<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/analysis/analyze_log.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/analysis/analyze_log.py" />
              <option name="originalContent" value="#!/usr/bin/env python3&#10;# -*- coding: utf-8 -*-&#10;&quot;&quot;&quot;&#10;分析扩散模型训练日志并生成图表&#10;直接使用文本内容进行分析&#10;&quot;&quot;&quot;&#10;&#10;from draw import analyze_training_from_text&#10;&#10;# 您提供的训练日志内容&#10;log_content = &quot;&quot;&quot;Loaded 12474 sequences for Diffusion model training (max_len=190).&#10;/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.&#10;  warnings.warn(&#10;Starting Diffusion Model training...&#10;Parameters: max_len=190, batch_size=64, epochs=200&#10;Model parameters: 427,797&#10;Early stopping: patience=30, min_improvement=0.001&#10;Regularization: dropout=0.7, weight_decay=0.005, label_smoothing=0.2&#10;Epoch 0 finished. Avg Loss: 1.8890, LR: 0.000030&#10;  → New best loss: 1.8890&#10;Epoch 1 finished. Avg Loss: 1.8227, LR: 0.000030&#10;  → New best loss: 1.8227&#10;Epoch 2 finished. Avg Loss: 1.8062, LR: 0.000030&#10;  → New best loss: 1.8062&#10;Epoch 3 finished. Avg Loss: 1.7851, LR: 0.000030&#10;  → New best loss: 1.7851&#10;Epoch 4 finished. Avg Loss: 1.7643, LR: 0.000030&#10;  → New best loss: 1.7643&#10;Epoch 5 finished. Avg Loss: 1.7687, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 6 finished. Avg Loss: 1.7676, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 7 finished. Avg Loss: 1.7581, LR: 0.000030&#10;  → New best loss: 1.7581&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9990&#10;  t=50: 0.8135&#10;  t=100: 0.3793&#10;  t=200: 0.1378&#10;[Sample] Example generated sequences:&#10;  1: DLEDVEIFTT&#10;  2: WMECHTTCMDHC&#10;  3: GCRGGRIDYMEFNRVLIYTRMTPMDTTGKYMPKSDWQCWVMNAATMPPMDNCRWECLSLIIYIMPWKWVPNVHPKIKARNLPNVY&#10;  4: EIKGSMKHPNTLQMQNHHQ&#10;[Diversity] Valid sequences: 26/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 27.0&#10;Epoch 8 finished. Avg Loss: 1.7423, LR: 0.000030&#10;  → New best loss: 1.7423&#10;Epoch 9 finished. Avg Loss: 1.7710, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 10 finished. Avg Loss: 1.7535, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 11 finished. Avg Loss: 1.7734, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 12 finished. Avg Loss: 1.7492, LR: 0.000030&#10;  → No improvement. Patience: 4/30&#10;Epoch 13 finished. Avg Loss: 1.7753, LR: 0.000030&#10;  → No improvement. Patience: 5/30&#10;Epoch 14 finished. Avg Loss: 1.7598, LR: 0.000030&#10;  → No improvement. Patience: 6/30&#10;Epoch 15 finished. Avg Loss: 1.7438, LR: 0.000030&#10;  → No improvement. Patience: 7/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 1.0000&#10;  t=50: 0.8191&#10;  t=100: 0.3888&#10;  t=200: 0.1434&#10;[Sample] Example generated sequences:&#10;  1: KAIMESPMFSCYKYC&#10;  2: LCGSVRHVWQARFIKHIYGK&#10;  3: SMSWALDVPQHTDDADWVWNTV&#10;  4: SMMAY&#10;[Diversity] Valid sequences: 31/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 21.3&#10;Epoch 16 finished. Avg Loss: 1.7402, LR: 0.000030&#10;  → New best loss: 1.7402&#10;Epoch 17 finished. Avg Loss: 1.7445, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 18 finished. Avg Loss: 1.7464, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 19 finished. Avg Loss: 1.7499, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 20 finished. Avg Loss: 1.7633, LR: 0.000030&#10;  → No improvement. Patience: 4/30&#10;Epoch 21 finished. Avg Loss: 1.7613, LR: 0.000030&#10;  → No improvement. Patience: 5/30&#10;Epoch 22 finished. Avg Loss: 1.7492, LR: 0.000030&#10;  → No improvement. Patience: 6/30&#10;Epoch 23 finished. Avg Loss: 1.7401, LR: 0.000030&#10;  → No improvement. Patience: 7/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9980&#10;  t=50: 0.8230&#10;  t=100: 0.3878&#10;  t=200: 0.1424&#10;[Sample] Example generated sequences:&#10;  1: RGCWMFTVAVLGCFC&#10;  2: EMMVGWNMERWRDRPDKEPGNYRDQVCQIFSV&#10;  3: MHCKEINII&#10;  4: PCNPPCATQGKMLW&#10;[Diversity] Valid sequences: 28/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 27.6&#10;Epoch 24 finished. Avg Loss: 1.7373, LR: 0.000030&#10;  → New best loss: 1.7373&#10;Epoch 25 finished. Avg Loss: 1.7344, LR: 0.000030&#10;  → New best loss: 1.7344&#10;Epoch 26 finished. Avg Loss: 1.7464, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 27 finished. Avg Loss: 1.7325, LR: 0.000030&#10;  → New best loss: 1.7325&#10;Epoch 28 finished. Avg Loss: 1.7369, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 29 finished. Avg Loss: 1.7212, LR: 0.000030&#10;  → New best loss: 1.7212&#10;Epoch 30 finished. Avg Loss: 1.7033, LR: 0.000030&#10;  → New best loss: 1.7033&#10;Epoch 31 finished. Avg Loss: 1.6966, LR: 0.000030&#10;  → New best loss: 1.6966&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9967&#10;  t=50: 0.7447&#10;  t=100: 0.3122&#10;  t=200: 0.1063&#10;[Sample] Example generated sequences:&#10;  1: LMLCTKEAQMGYMEMFQNWVH&#10;  2: WKMVKWPWEYDWK&#10;  3: MNKNAEDFDTLN&#10;  4: QDFRQGQ&#10;[Diversity] Valid sequences: 22/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 14.2&#10;Epoch 32 finished. Avg Loss: 1.7223, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 33 finished. Avg Loss: 1.7032, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 34 finished. Avg Loss: 1.6861, LR: 0.000030&#10;  → New best loss: 1.6861&#10;Epoch 35 finished. Avg Loss: 1.6763, LR: 0.000030&#10;  → New best loss: 1.6763&#10;Epoch 36 finished. Avg Loss: 1.6579, LR: 0.000030&#10;  → New best loss: 1.6579&#10;Epoch 37 finished. Avg Loss: 1.6559, LR: 0.000030&#10;  → New best loss: 1.6559&#10;Epoch 38 finished. Avg Loss: 1.6699, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 39 finished. Avg Loss: 1.6417, LR: 0.000030&#10;  → New best loss: 1.6417&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9934&#10;  t=50: 0.7625&#10;  t=100: 0.3349&#10;  t=200: 0.1316&#10;[Sample] Example generated sequences:&#10;  1: MCLCCCHTESFIADVNVSMHTSLFQGYIPSHHWFSHVMMDIDGIFISCPPYVDEC&#10;  2: PQFHN&#10;  3: RNSKPWWK&#10;  4: CRDCEQEHAVPTL&#10;[Diversity] Valid sequences: 26/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 17.5&#10;Epoch 40 finished. Avg Loss: 1.6271, LR: 0.000030&#10;  → New best loss: 1.6271&#10;Epoch 41 finished. Avg Loss: 1.6191, LR: 0.000030&#10;  → New best loss: 1.6191&#10;Epoch 42 finished. Avg Loss: 1.6341, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 43 finished. Avg Loss: 1.6153, LR: 0.000030&#10;  → New best loss: 1.6153&#10;Epoch 44 finished. Avg Loss: 1.6280, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 45 finished. Avg Loss: 1.6114, LR: 0.000030&#10;  → New best loss: 1.6114&#10;Epoch 46 finished. Avg Loss: 1.6116, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 47 finished. Avg Loss: 1.6094, LR: 0.000030&#10;  → New best loss: 1.6094&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9934&#10;  t=50: 0.7563&#10;  t=100: 0.3431&#10;  t=200: 0.1257&#10;[Sample] Example generated sequences:&#10;  1: LAGMNV&#10;  2: IEGDDDM&#10;  3: PINTTTHPRSAFGNDRYPVSSGFW&#10;  4: RGEGRNDILVIQVQSDADVPPIDASPGHDPIPAISLMVALSINSV&#10;[Diversity] Valid sequences: 28/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 18.8&#10;Epoch 48 finished. Avg Loss: 1.6222, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 49 finished. Avg Loss: 1.6191, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 50 finished. Avg Loss: 1.6168, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 51 finished. Avg Loss: 1.6113, LR: 0.000030&#10;  → No improvement. Patience: 4/30&#10;Epoch 52 finished. Avg Loss: 1.6063, LR: 0.000030&#10;  → New best loss: 1.6063&#10;Epoch 53 finished. Avg Loss: 1.6052, LR: 0.000030&#10;  → New best loss: 1.6052&#10;Epoch 54 finished. Avg Loss: 1.6080, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 55 finished. Avg Loss: 1.6122, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9961&#10;  t=50: 0.7678&#10;  t=100: 0.3477&#10;  t=200: 0.1418&#10;[Sample] Example generated sequences:&#10;  1: TQMKTDIV&#10;  2: WDYVGHPHTI&#10;  3: KHHNICRHTRWYLPITLLI&#10;  4: SFKGKLGERDIR&#10;[Diversity] Valid sequences: 21/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 17.5&#10;Epoch 56 finished. Avg Loss: 1.6090, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 57 finished. Avg Loss: 1.5946, LR: 0.000030&#10;  → New best loss: 1.5946&#10;Epoch 58 finished. Avg Loss: 1.6042, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 59 finished. Avg Loss: 1.6174, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 60 finished. Avg Loss: 1.6150, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 61 finished. Avg Loss: 1.6249, LR: 0.000030&#10;  → No improvement. Patience: 4/30&#10;Epoch 62 finished. Avg Loss: 1.6184, LR: 0.000030&#10;  → No improvement. Patience: 5/30&#10;Epoch 63 finished. Avg Loss: 1.6205, LR: 0.000030&#10;  → No improvement. Patience: 6/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9941&#10;  t=50: 0.7618&#10;  t=100: 0.3454&#10;  t=200: 0.1188&#10;[Sample] Example generated sequences:&#10;  1: FAQRDKGGFMFPLCPGDCEQF&#10;  2: TFSHEVKFYDVPGATTWMFFVSLHPVPYKG&#10;  3: HVAEKVKTMWVSGNMMGSKMTQFC&#10;  4: WMFDEQDRWVRPSCWGGGNPFAN&#10;[Diversity] Valid sequences: 23/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 19.7&#10;Epoch 64 finished. Avg Loss: 1.6215, LR: 0.000030&#10;  → No improvement. Patience: 7/30&#10;Epoch 65 finished. Avg Loss: 1.6119, LR: 0.000030&#10;  → No improvement. Patience: 8/30&#10;Epoch 66 finished. Avg Loss: 1.5938, LR: 0.000030&#10;  → No improvement. Patience: 9/30&#10;Epoch 67 finished. Avg Loss: 1.6112, LR: 0.000030&#10;  → No improvement. Patience: 10/30&#10;Epoch 68 finished. Avg Loss: 1.6026, LR: 0.000030&#10;  → No improvement. Patience: 11/30&#10;Epoch 69 finished. Avg Loss: 1.5972, LR: 0.000030&#10;  → No improvement. Patience: 12/30&#10;Epoch 70 finished. Avg Loss: 1.6143, LR: 0.000030&#10;  → No improvement. Patience: 13/30&#10;Epoch 71 finished. Avg Loss: 1.6064, LR: 0.000030&#10;  → No improvement. Patience: 14/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9974&#10;  t=50: 0.7553&#10;  t=100: 0.3345&#10;  t=200: 0.1132&#10;[Sample] Example generated sequences:&#10;  1: DQCNEMNYTEKDNGDMKPQEAEYDADFCKEGMYY&#10;  2: RKDCFYPRPVFKVSMKVTGESL&#10;  3: YGFDFGHLCYLEKCVDMSFILFVWGYG&#10;  4: YSCHWWKTYDDCDHHYQNW&#10;[Diversity] Valid sequences: 24/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 21.4&#10;Epoch 72 finished. Avg Loss: 1.6114, LR: 0.000030&#10;  → No improvement. Patience: 15/30&#10;Epoch 73 finished. Avg Loss: 1.5965, LR: 0.000030&#10;  → No improvement. Patience: 16/30&#10;Epoch 74 finished. Avg Loss: 1.5853, LR: 0.000030&#10;  → New best loss: 1.5853&#10;Epoch 75 finished. Avg Loss: 1.5989, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 76 finished. Avg Loss: 1.5979, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 77 finished. Avg Loss: 1.6049, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 78 finished. Avg Loss: 1.6080, LR: 0.000030&#10;  → No improvement. Patience: 4/30&#10;Epoch 79 finished. Avg Loss: 1.5980, LR: 0.000030&#10;  → No improvement. Patience: 5/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9977&#10;  t=50: 0.7684&#10;  t=100: 0.3559&#10;  t=200: 0.1204&#10;[Sample] Example generated sequences:&#10;  1: AWSTANVK&#10;  2: DMAKPMNQVHISC&#10;  3: MESTHEMT&#10;  4: CEDKLAASCGHWYMFL&#10;[Diversity] Valid sequences: 23/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 23.7&#10;Epoch 80 finished. Avg Loss: 1.6026, LR: 0.000030&#10;  → No improvement. Patience: 6/30&#10;Epoch 81 finished. Avg Loss: 1.6054, LR: 0.000030&#10;  → No improvement. Patience: 7/30&#10;Epoch 82 finished. Avg Loss: 1.6072, LR: 0.000030&#10;  → No improvement. Patience: 8/30&#10;Epoch 83 finished. Avg Loss: 1.6016, LR: 0.000015&#10;  → No improvement. Patience: 9/30&#10;Epoch 84 finished. Avg Loss: 1.5953, LR: 0.000015&#10;  → No improvement. Patience: 10/30&#10;Epoch 85 finished. Avg Loss: 1.6026, LR: 0.000015&#10;  → No improvement. Patience: 11/30&#10;Epoch 86 finished. Avg Loss: 1.6110, LR: 0.000015&#10;  → No improvement. Patience: 12/30&#10;Epoch 87 finished. Avg Loss: 1.5937, LR: 0.000015&#10;  → No improvement. Patience: 13/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9957&#10;  t=50: 0.7770&#10;  t=100: 0.3684&#10;  t=200: 0.1276&#10;[Sample] Example generated sequences:&#10;  1: QKKVQTWCGFICFHMVCRVINLRVIAVSTLQREETVAMRWYTF&#10;  2: DKVPTMNPMRGIPHKK&#10;  3: AIMLVHVGPDLKPMPKEGLHNTWPPKVDSKNYCDKVMISMRCPWVVTRNGI&#10;  4: LNMCSPSHVILEMN&#10;[Diversity] Valid sequences: 27/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 24.1&#10;Epoch 88 finished. Avg Loss: 1.6073, LR: 0.000015&#10;  → No improvement. Patience: 14/30&#10;Epoch 89 finished. Avg Loss: 1.5993, LR: 0.000015&#10;  → No improvement. Patience: 15/30&#10;Epoch 90 finished. Avg Loss: 1.6093, LR: 0.000015&#10;  → No improvement. Patience: 16/30&#10;Epoch 91 finished. Avg Loss: 1.5979, LR: 0.000015&#10;  → No improvement. Patience: 17/30&#10;Epoch 92 finished. Avg Loss: 1.6166, LR: 0.000008&#10;  → No improvement. Patience: 18/30&#10;Epoch 93 finished. Avg Loss: 1.6016, LR: 0.000008&#10;  → No improvement. Patience: 19/30&#10;Epoch 94 finished. Avg Loss: 1.6022, LR: 0.000008&#10;  → No improvement. Patience: 20/30&#10;Epoch 95 finished. Avg Loss: 1.6059, LR: 0.000008&#10;  → No improvement. Patience: 21/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9954&#10;  t=50: 0.7641&#10;  t=100: 0.3490&#10;  t=200: 0.1178&#10;[Sample] Example generated sequences:&#10;  1: VCNPMCVAGGH&#10;  2: DDDAGTARIQYDKGEVNTLMNNNMMMTKHEIGGWGEWNTGQWTYDTETAILKCKEPS&#10;  3: MEQTGSEWILF&#10;  4: KCEINLFSFHEKGLKDSM&#10;[Diversity] Valid sequences: 23/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 21.8&#10;Epoch 96 finished. Avg Loss: 1.6087, LR: 0.000008&#10;  → No improvement. Patience: 22/30&#10;Epoch 97 finished. Avg Loss: 1.6074, LR: 0.000008&#10;  → No improvement. Patience: 23/30&#10;Epoch 98 finished. Avg Loss: 1.6074, LR: 0.000008&#10;  → No improvement. Patience: 24/30&#10;Epoch 99 finished. Avg Loss: 1.6034, LR: 0.000008&#10;  → No improvement. Patience: 25/30&#10;Epoch 100 finished. Avg Loss: 1.5989, LR: 0.000008&#10;  → No improvement. Patience: 26/30&#10;Epoch 101 finished. Avg Loss: 1.6133, LR: 0.000004&#10;  → No improvement. Patience: 27/30&#10;Epoch 102 finished. Avg Loss: 1.5959, LR: 0.000004&#10;  → No improvement. Patience: 28/30&#10;Epoch 103 finished. Avg Loss: 1.5965, LR: 0.000004&#10;  → No improvement. Patience: 29/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9974&#10;  t=50: 0.7734&#10;  t=100: 0.3457&#10;  t=200: 0.1329&#10;[Sample] Example generated sequences:&#10;  1: KMSDHPHNHTNKTKRTRGTINKAQTAVPSKYFEQKDALRYAFTVYQGDGGPNPFPKHRCDKKE&#10;  2: CLQDPVS&#10;  3: WLYFPMFTSGTNDMCRSITPTWINTEDNMLKDC&#10;  4: DQYSFCMYMSS&#10;[Diversity] Valid sequences: 23/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 26.5&#10;Epoch 104 finished. Avg Loss: 1.6063, LR: 0.000004&#10;  → No improvement. Patience: 30/30&#10;&#10;Early stopping triggered after 105 epochs&#10;Best loss: 1.5853&#10;Loaded best model with loss: 1.5853&#10;&#10;Training finished. Model saved to models/diffusion_model_transformer.pth&#10;Total training time: 591.49 seconds.&quot;&quot;&quot;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    print(&quot;开始分析扩散模型训练日志...&quot;)&#10;    print(&quot;=&quot; * 60)&#10;&#10;    # 分析训练日志并生成图表&#10;    log_data = analyze_training_from_text(log_content, &quot;./results/&quot;)&#10;&#10;    print(&quot;\n分析完成！生成的文件包括：&quot;)&#10;    print(&quot;1. training_metrics.png - 训练过程综合指标图&quot;)&#10;    print(&quot;2. reconstruction_heatmap.png - 重构准确率热力图&quot;)&#10;    print(&quot;3. early_stopping_analysis.png - 早停机制分析图&quot;)&#10;    print(&quot;4. training_summary.txt - 训练总结报告&quot;)&#10;&#10;    print(&quot;\n这些图表可以直接用于论文中！&quot;)&#10;" />
              <option name="updatedContent" value="#!/usr/bin/env python3&#10;# -*- coding: utf-8 -*-&#10;&quot;&quot;&quot;&#10;分析扩散模型训练日志并生成图表&#10;直接使用文本内容进行分析&#10;&quot;&quot;&quot;&#10;&#10;from draw import analyze_training_from_text&#10;&#10;# 您提供的训练日志内容&#10;log_content = &quot;&quot;&quot;Loaded 12474 sequences for Diffusion model training (max_len=190).&#10;/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.&#10;  warnings.warn(&#10;Starting Diffusion Model training...&#10;Parameters: max_len=190, batch_size=64, epochs=200&#10;Model parameters: 427,797&#10;Early stopping: patience=30, min_improvement=0.001&#10;Regularization: dropout=0.7, weight_decay=0.005, label_smoothing=0.2&#10;Epoch 0 finished. Avg Loss: 1.8890, LR: 0.000030&#10;  → New best loss: 1.8890&#10;Epoch 1 finished. Avg Loss: 1.8227, LR: 0.000030&#10;  → New best loss: 1.8227&#10;Epoch 2 finished. Avg Loss: 1.8062, LR: 0.000030&#10;  → New best loss: 1.8062&#10;Epoch 3 finished. Avg Loss: 1.7851, LR: 0.000030&#10;  → New best loss: 1.7851&#10;Epoch 4 finished. Avg Loss: 1.7643, LR: 0.000030&#10;  → New best loss: 1.7643&#10;Epoch 5 finished. Avg Loss: 1.7687, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 6 finished. Avg Loss: 1.7676, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 7 finished. Avg Loss: 1.7581, LR: 0.000030&#10;  → New best loss: 1.7581&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9990&#10;  t=50: 0.8135&#10;  t=100: 0.3793&#10;  t=200: 0.1378&#10;[Sample] Example generated sequences:&#10;  1: DLEDVEIFTT&#10;  2: WMECHTTCMDHC&#10;  3: GCRGGRIDYMEFNRVLIYTRMTPMDTTGKYMPKSDWQCWVMNAATMPPMDNCRWECLSLIIYIMPWKWVPNVHPKIKARNLPNVY&#10;  4: EIKGSMKHPNTLQMQNHHQ&#10;[Diversity] Valid sequences: 26/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 27.0&#10;Epoch 8 finished. Avg Loss: 1.7423, LR: 0.000030&#10;  → New best loss: 1.7423&#10;Epoch 9 finished. Avg Loss: 1.7710, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 10 finished. Avg Loss: 1.7535, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 11 finished. Avg Loss: 1.7734, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 12 finished. Avg Loss: 1.7492, LR: 0.000030&#10;  → No improvement. Patience: 4/30&#10;Epoch 13 finished. Avg Loss: 1.7753, LR: 0.000030&#10;  → No improvement. Patience: 5/30&#10;Epoch 14 finished. Avg Loss: 1.7598, LR: 0.000030&#10;  → No improvement. Patience: 6/30&#10;Epoch 15 finished. Avg Loss: 1.7438, LR: 0.000030&#10;  → No improvement. Patience: 7/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 1.0000&#10;  t=50: 0.8191&#10;  t=100: 0.3888&#10;  t=200: 0.1434&#10;[Sample] Example generated sequences:&#10;  1: KAIMESPMFSCYKYC&#10;  2: LCGSVRHVWQARFIKHIYGK&#10;  3: SMSWALDVPQHTDDADWVWNTV&#10;  4: SMMAY&#10;[Diversity] Valid sequences: 31/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 21.3&#10;Epoch 16 finished. Avg Loss: 1.7402, LR: 0.000030&#10;  → New best loss: 1.7402&#10;Epoch 17 finished. Avg Loss: 1.7445, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 18 finished. Avg Loss: 1.7464, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 19 finished. Avg Loss: 1.7499, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 20 finished. Avg Loss: 1.7633, LR: 0.000030&#10;  → No improvement. Patience: 4/30&#10;Epoch 21 finished. Avg Loss: 1.7613, LR: 0.000030&#10;  → No improvement. Patience: 5/30&#10;Epoch 22 finished. Avg Loss: 1.7492, LR: 0.000030&#10;  → No improvement. Patience: 6/30&#10;Epoch 23 finished. Avg Loss: 1.7401, LR: 0.000030&#10;  → No improvement. Patience: 7/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9980&#10;  t=50: 0.8230&#10;  t=100: 0.3878&#10;  t=200: 0.1424&#10;[Sample] Example generated sequences:&#10;  1: RGCWMFTVAVLGCFC&#10;  2: EMMVGWNMERWRDRPDKEPGNYRDQVCQIFSV&#10;  3: MHCKEINII&#10;  4: PCNPPCATQGKMLW&#10;[Diversity] Valid sequences: 28/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 27.6&#10;Epoch 24 finished. Avg Loss: 1.7373, LR: 0.000030&#10;  → New best loss: 1.7373&#10;Epoch 25 finished. Avg Loss: 1.7344, LR: 0.000030&#10;  → New best loss: 1.7344&#10;Epoch 26 finished. Avg Loss: 1.7464, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 27 finished. Avg Loss: 1.7325, LR: 0.000030&#10;  → New best loss: 1.7325&#10;Epoch 28 finished. Avg Loss: 1.7369, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 29 finished. Avg Loss: 1.7212, LR: 0.000030&#10;  → New best loss: 1.7212&#10;Epoch 30 finished. Avg Loss: 1.7033, LR: 0.000030&#10;  → New best loss: 1.7033&#10;Epoch 31 finished. Avg Loss: 1.6966, LR: 0.000030&#10;  → New best loss: 1.6966&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9967&#10;  t=50: 0.7447&#10;  t=100: 0.3122&#10;  t=200: 0.1063&#10;[Sample] Example generated sequences:&#10;  1: LMLCTKEAQMGYMEMFQNWVH&#10;  2: WKMVKWPWEYDWK&#10;  3: MNKNAEDFDTLN&#10;  4: QDFRQGQ&#10;[Diversity] Valid sequences: 22/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 14.2&#10;Epoch 32 finished. Avg Loss: 1.7223, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 33 finished. Avg Loss: 1.7032, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 34 finished. Avg Loss: 1.6861, LR: 0.000030&#10;  → New best loss: 1.6861&#10;Epoch 35 finished. Avg Loss: 1.6763, LR: 0.000030&#10;  → New best loss: 1.6763&#10;Epoch 36 finished. Avg Loss: 1.6579, LR: 0.000030&#10;  → New best loss: 1.6579&#10;Epoch 37 finished. Avg Loss: 1.6559, LR: 0.000030&#10;  → New best loss: 1.6559&#10;Epoch 38 finished. Avg Loss: 1.6699, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 39 finished. Avg Loss: 1.6417, LR: 0.000030&#10;  → New best loss: 1.6417&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9934&#10;  t=50: 0.7625&#10;  t=100: 0.3349&#10;  t=200: 0.1316&#10;[Sample] Example generated sequences:&#10;  1: MCLCCCHTESFIADVNVSMHTSLFQGYIPSHHWFSHVMMDIDGIFISCPPYVDEC&#10;  2: PQFHN&#10;  3: RNSKPWWK&#10;  4: CRDCEQEHAVPTL&#10;[Diversity] Valid sequences: 26/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 17.5&#10;Epoch 40 finished. Avg Loss: 1.6271, LR: 0.000030&#10;  → New best loss: 1.6271&#10;Epoch 41 finished. Avg Loss: 1.6191, LR: 0.000030&#10;  → New best loss: 1.6191&#10;Epoch 42 finished. Avg Loss: 1.6341, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 43 finished. Avg Loss: 1.6153, LR: 0.000030&#10;  → New best loss: 1.6153&#10;Epoch 44 finished. Avg Loss: 1.6280, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 45 finished. Avg Loss: 1.6114, LR: 0.000030&#10;  → New best loss: 1.6114&#10;Epoch 46 finished. Avg Loss: 1.6116, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 47 finished. Avg Loss: 1.6094, LR: 0.000030&#10;  → New best loss: 1.6094&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9934&#10;  t=50: 0.7563&#10;  t=100: 0.3431&#10;  t=200: 0.1257&#10;[Sample] Example generated sequences:&#10;  1: LAGMNV&#10;  2: IEGDDDM&#10;  3: PINTTTHPRSAFGNDRYPVSSGFW&#10;  4: RGEGRNDILVIQVQSDADVPPIDASPGHDPIPAISLMVALSINSV&#10;[Diversity] Valid sequences: 28/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 18.8&#10;Epoch 48 finished. Avg Loss: 1.6222, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 49 finished. Avg Loss: 1.6191, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 50 finished. Avg Loss: 1.6168, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 51 finished. Avg Loss: 1.6113, LR: 0.000030&#10;  → No improvement. Patience: 4/30&#10;Epoch 52 finished. Avg Loss: 1.6063, LR: 0.000030&#10;  → New best loss: 1.6063&#10;Epoch 53 finished. Avg Loss: 1.6052, LR: 0.000030&#10;  → New best loss: 1.6052&#10;Epoch 54 finished. Avg Loss: 1.6080, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 55 finished. Avg Loss: 1.6122, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9961&#10;  t=50: 0.7678&#10;  t=100: 0.3477&#10;  t=200: 0.1418&#10;[Sample] Example generated sequences:&#10;  1: TQMKTDIV&#10;  2: WDYVGHPHTI&#10;  3: KHHNICRHTRWYLPITLLI&#10;  4: SFKGKLGERDIR&#10;[Diversity] Valid sequences: 21/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 17.5&#10;Epoch 56 finished. Avg Loss: 1.6090, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 57 finished. Avg Loss: 1.5946, LR: 0.000030&#10;  → New best loss: 1.5946&#10;Epoch 58 finished. Avg Loss: 1.6042, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 59 finished. Avg Loss: 1.6174, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 60 finished. Avg Loss: 1.6150, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 61 finished. Avg Loss: 1.6249, LR: 0.000030&#10;  → No improvement. Patience: 4/30&#10;Epoch 62 finished. Avg Loss: 1.6184, LR: 0.000030&#10;  → No improvement. Patience: 5/30&#10;Epoch 63 finished. Avg Loss: 1.6205, LR: 0.000030&#10;  → No improvement. Patience: 6/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9941&#10;  t=50: 0.7618&#10;  t=100: 0.3454&#10;  t=200: 0.1188&#10;[Sample] Example generated sequences:&#10;  1: FAQRDKGGFMFPLCPGDCEQF&#10;  2: TFSHEVKFYDVPGATTWMFFVSLHPVPYKG&#10;  3: HVAEKVKTMWVSGNMMGSKMTQFC&#10;  4: WMFDEQDRWVRPSCWGGGNPFAN&#10;[Diversity] Valid sequences: 23/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 19.7&#10;Epoch 64 finished. Avg Loss: 1.6215, LR: 0.000030&#10;  → No improvement. Patience: 7/30&#10;Epoch 65 finished. Avg Loss: 1.6119, LR: 0.000030&#10;  → No improvement. Patience: 8/30&#10;Epoch 66 finished. Avg Loss: 1.5938, LR: 0.000030&#10;  → No improvement. Patience: 9/30&#10;Epoch 67 finished. Avg Loss: 1.6112, LR: 0.000030&#10;  → No improvement. Patience: 10/30&#10;Epoch 68 finished. Avg Loss: 1.6026, LR: 0.000030&#10;  → No improvement. Patience: 11/30&#10;Epoch 69 finished. Avg Loss: 1.5972, LR: 0.000030&#10;  → No improvement. Patience: 12/30&#10;Epoch 70 finished. Avg Loss: 1.6143, LR: 0.000030&#10;  → No improvement. Patience: 13/30&#10;Epoch 71 finished. Avg Loss: 1.6064, LR: 0.000030&#10;  → No improvement. Patience: 14/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9974&#10;  t=50: 0.7553&#10;  t=100: 0.3345&#10;  t=200: 0.1132&#10;[Sample] Example generated sequences:&#10;  1: DQCNEMNYTEKDNGDMKPQEAEYDADFCKEGMYY&#10;  2: RKDCFYPRPVFKVSMKVTGESL&#10;  3: YGFDFGHLCYLEKCVDMSFILFVWGYG&#10;  4: YSCHWWKTYDDCDHHYQNW&#10;[Diversity] Valid sequences: 24/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 21.4&#10;Epoch 72 finished. Avg Loss: 1.6114, LR: 0.000030&#10;  → No improvement. Patience: 15/30&#10;Epoch 73 finished. Avg Loss: 1.5965, LR: 0.000030&#10;  → No improvement. Patience: 16/30&#10;Epoch 74 finished. Avg Loss: 1.5853, LR: 0.000030&#10;  → New best loss: 1.5853&#10;Epoch 75 finished. Avg Loss: 1.5989, LR: 0.000030&#10;  → No improvement. Patience: 1/30&#10;Epoch 76 finished. Avg Loss: 1.5979, LR: 0.000030&#10;  → No improvement. Patience: 2/30&#10;Epoch 77 finished. Avg Loss: 1.6049, LR: 0.000030&#10;  → No improvement. Patience: 3/30&#10;Epoch 78 finished. Avg Loss: 1.6080, LR: 0.000030&#10;  → No improvement. Patience: 4/30&#10;Epoch 79 finished. Avg Loss: 1.5980, LR: 0.000030&#10;  → No improvement. Patience: 5/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9977&#10;  t=50: 0.7684&#10;  t=100: 0.3559&#10;  t=200: 0.1204&#10;[Sample] Example generated sequences:&#10;  1: AWSTANVK&#10;  2: DMAKPMNQVHISC&#10;  3: MESTHEMT&#10;  4: CEDKLAASCGHWYMFL&#10;[Diversity] Valid sequences: 23/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 23.7&#10;Epoch 80 finished. Avg Loss: 1.6026, LR: 0.000030&#10;  → No improvement. Patience: 6/30&#10;Epoch 81 finished. Avg Loss: 1.6054, LR: 0.000030&#10;  → No improvement. Patience: 7/30&#10;Epoch 82 finished. Avg Loss: 1.6072, LR: 0.000030&#10;  → No improvement. Patience: 8/30&#10;Epoch 83 finished. Avg Loss: 1.6016, LR: 0.000015&#10;  → No improvement. Patience: 9/30&#10;Epoch 84 finished. Avg Loss: 1.5953, LR: 0.000015&#10;  → No improvement. Patience: 10/30&#10;Epoch 85 finished. Avg Loss: 1.6026, LR: 0.000015&#10;  → No improvement. Patience: 11/30&#10;Epoch 86 finished. Avg Loss: 1.6110, LR: 0.000015&#10;  → No improvement. Patience: 12/30&#10;Epoch 87 finished. Avg Loss: 1.5937, LR: 0.000015&#10;  → No improvement. Patience: 13/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9957&#10;  t=50: 0.7770&#10;  t=100: 0.3684&#10;  t=200: 0.1276&#10;[Sample] Example generated sequences:&#10;  1: QKKVQTWCGFICFHMVCRVINLRVIAVSTLQREETVAMRWYTF&#10;  2: DKVPTMNPMRGIPHKK&#10;  3: AIMLVHVGPDLKPMPKEGLHNTWPPKVDSKNYCDKVMISMRCPWVVTRNGI&#10;  4: LNMCSPSHVILEMN&#10;[Diversity] Valid sequences: 27/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 24.1&#10;Epoch 88 finished. Avg Loss: 1.6073, LR: 0.000015&#10;  → No improvement. Patience: 14/30&#10;Epoch 89 finished. Avg Loss: 1.5993, LR: 0.000015&#10;  → No improvement. Patience: 15/30&#10;Epoch 90 finished. Avg Loss: 1.6093, LR: 0.000015&#10;  → No improvement. Patience: 16/30&#10;Epoch 91 finished. Avg Loss: 1.5979, LR: 0.000015&#10;  → No improvement. Patience: 17/30&#10;Epoch 92 finished. Avg Loss: 1.6166, LR: 0.000008&#10;  → No improvement. Patience: 18/30&#10;Epoch 93 finished. Avg Loss: 1.6016, LR: 0.000008&#10;  → No improvement. Patience: 19/30&#10;Epoch 94 finished. Avg Loss: 1.6022, LR: 0.000008&#10;  → No improvement. Patience: 20/30&#10;Epoch 95 finished. Avg Loss: 1.6059, LR: 0.000008&#10;  → No improvement. Patience: 21/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9954&#10;  t=50: 0.7641&#10;  t=100: 0.3490&#10;  t=200: 0.1178&#10;[Sample] Example generated sequences:&#10;  1: VCNPMCVAGGH&#10;  2: DDDAGTARIQYDKGEVNTLMNNNMMMTKHEIGGWGEWNTGQWTYDTETAILKCKEPS&#10;  3: MEQTGSEWILF&#10;  4: KCEINLFSFHEKGLKDSM&#10;[Diversity] Valid sequences: 23/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 21.8&#10;Epoch 96 finished. Avg Loss: 1.6087, LR: 0.000008&#10;  → No improvement. Patience: 22/30&#10;Epoch 97 finished. Avg Loss: 1.6074, LR: 0.000008&#10;  → No improvement. Patience: 23/30&#10;Epoch 98 finished. Avg Loss: 1.6074, LR: 0.000008&#10;  → No improvement. Patience: 24/30&#10;Epoch 99 finished. Avg Loss: 1.6034, LR: 0.000008&#10;  → No improvement. Patience: 25/30&#10;Epoch 100 finished. Avg Loss: 1.5989, LR: 0.000008&#10;  → No improvement. Patience: 26/30&#10;Epoch 101 finished. Avg Loss: 1.6133, LR: 0.000004&#10;  → No improvement. Patience: 27/30&#10;Epoch 102 finished. Avg Loss: 1.5959, LR: 0.000004&#10;  → No improvement. Patience: 28/30&#10;Epoch 103 finished. Avg Loss: 1.5965, LR: 0.000004&#10;  → No improvement. Patience: 29/30&#10;[Eval] Reconstruction Accuracy:&#10;  t=20: 0.9974&#10;  t=50: 0.7734&#10;  t=100: 0.3457&#10;  t=200: 0.1329&#10;[Sample] Example generated sequences:&#10;  1: KMSDHPHNHTNKTKRTRGTINKAQTAVPSKYFEQKDALRYAFTVYQGDGGPNPFPKHRCDKKE&#10;  2: CLQDPVS&#10;  3: WLYFPMFTSGTNDMCRSITPTWINTEDNMLKDC&#10;  4: DQYSFCMYMSS&#10;[Diversity] Valid sequences: 23/32&#10;[Diversity] Generation diversity: 1.0000, Avg length: 26.5&#10;Epoch 104 finished. Avg Loss: 1.6063, LR: 0.000004&#10;  → No improvement. Patience: 30/30&#10;&#10;Early stopping triggered after 105 epochs&#10;Best loss: 1.5853&#10;Loaded best model with loss: 1.5853&#10;&#10;Training finished. Model saved to models/diffusion_model_transformer.pth&#10;Total training time: 591.49 seconds.&quot;&quot;&quot;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    print(&quot;开始分析扩散模型训练日志...&quot;)&#10;    print(&quot;=&quot; * 60)&#10;&#10;    # 分析训练日志并生成图表&#10;    log_data = analyze_training_from_text(log_content, &quot;./results/&quot;)&#10;&#10;    print(&quot;\n分析完成！生成的文件包括：&quot;)&#10;    print(&quot;1. training_metrics.png - 训练过程综合指标图&quot;)&#10;    print(&quot;2. reconstruction_heatmap.png - 重构准确率热力图&quot;)&#10;    print(&quot;3. early_stopping_analysis.png - 早停机制分析图&quot;)&#10;    print(&quot;4. training_summary.txt - 训练总结报告&quot;)&#10;&#10;    print(&quot;\n这些图表可以直接用于论文中！&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/analysis/draw.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/analysis/draw.py" />
              <option name="originalContent" value="import pandas as pd&#10;import matplotlib.pyplot as plt&#10;import numpy as np&#10;import seaborn as sns&#10;from typing import List, Dict, Tuple&#10;import re&#10;&#10;# 设置中文字体和绘图风格&#10;plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']&#10;plt.rcParams['axes.unicode_minus'] = False&#10;sns.set_style(&quot;whitegrid&quot;)&#10;plt.rcParams['figure.dpi'] = 300&#10;&#10;&#10;def plot_amp_length_distribution(csv_path):&#10;    &quot;&quot;&quot;绘制AMP长度分布图&quot;&quot;&quot;&#10;    df = pd.read_csv(csv_path)&#10;    df_pos = df[df['label'] == 1]&#10;&#10;    if 'sequence' not in df_pos.columns:&#10;        raise ValueError(&quot;CSV文件缺少'sequence'列&quot;)&#10;&#10;    lengths = df_pos['sequence'].apply(len)&#10;    lengths = lengths[(lengths &gt;= 6) &amp; (lengths &lt;= 75)]&#10;    length_counts = lengths.value_counts().sort_index()&#10;&#10;    plt.figure(figsize=(12, 6))&#10;    plt.bar(length_counts.index, length_counts.values, color='skyblue', alpha=0.7, edgecolor='navy')&#10;    plt.xlabel('肽的长度（氨基酸个数）', fontsize=12)&#10;    plt.ylabel('数量', fontsize=12)&#10;    plt.title('阳性样本AMP长度分布', fontsize=14, fontweight='bold')&#10;    plt.grid(True, alpha=0.3)&#10;    plt.tight_layout()&#10;    plt.show()&#10;&#10;&#10;def parse_training_log(log_file_path: str) -&gt; Dict:&#10;    &quot;&quot;&quot;解析训练日志文件，提取关键指标&quot;&quot;&quot;&#10;    with open(log_file_path, 'r', encoding='utf-8') as f:&#10;        content = f.read()&#10;&#10;    # 提取训练数据&#10;    epochs = []&#10;    losses = []&#10;    learning_rates = []&#10;    patience_counts = []&#10;&#10;    # 重构准确率数据&#10;    recon_data = {&#10;        'epoch': [],&#10;        't20': [], 't50': [], 't100': [], 't200': []&#10;    }&#10;&#10;    # 多样性数据&#10;    diversity_data = {&#10;        'epoch': [],&#10;        'valid_sequences': [],&#10;        'total_sequences': [],&#10;        'diversity_score': [],&#10;        'avg_length': []&#10;    }&#10;&#10;    lines = content.split('\n')&#10;    current_epoch = None&#10;&#10;    for line in lines:&#10;        # 提取epoch、loss、learning rate&#10;        epoch_match = re.search(r'Epoch (\d+) finished\. Avg Loss: ([\d.]+), LR: ([\d.e-]+)', line)&#10;        if epoch_match:&#10;            current_epoch = int(epoch_match.group(1))&#10;            epochs.append(current_epoch)&#10;            losses.append(float(epoch_match.group(2)))&#10;            learning_rates.append(float(epoch_match.group(3)))&#10;&#10;        # 提取patience计数&#10;        patience_match = re.search(r'Patience: (\d+)/(\d+)', line)&#10;        if patience_match and current_epoch is not None:&#10;            patience_counts.append((current_epoch, int(patience_match.group(1))))&#10;&#10;        # 提取重构准确率&#10;        if 'Reconstruction Accuracy:' in line and current_epoch is not None:&#10;            recon_data['epoch'].append(current_epoch)&#10;&#10;        recon_match = re.search(r't=(\d+): ([\d.]+)', line)&#10;        if recon_match and current_epoch in recon_data['epoch']:&#10;            t_val = recon_match.group(1)&#10;            acc_val = float(recon_match.group(2))&#10;            if t_val in ['20', '50', '100', '200']:&#10;                recon_data[f't{t_val}'].append(acc_val)&#10;&#10;        # 提取多样性数据&#10;        diversity_match = re.search(r'Valid sequences: (\d+)/(\d+)', line)&#10;        if diversity_match and current_epoch is not None:&#10;            valid_seq = int(diversity_match.group(1))&#10;            total_seq = int(diversity_match.group(2))&#10;            diversity_data['epoch'].append(current_epoch)&#10;            diversity_data['valid_sequences'].append(valid_seq)&#10;            diversity_data['total_sequences'].append(total_seq)&#10;&#10;        diversity_score_match = re.search(r'Generation diversity: ([\d.]+), Avg length: ([\d.]+)', line)&#10;        if diversity_score_match and len(diversity_data['diversity_score']) &lt; len(diversity_data['epoch']):&#10;            diversity_data['diversity_score'].append(float(diversity_score_match.group(1)))&#10;            diversity_data['avg_length'].append(float(diversity_score_match.group(2)))&#10;&#10;    return {&#10;        'epochs': epochs,&#10;        'losses': losses,&#10;        'learning_rates': learning_rates,&#10;        'patience_counts': patience_counts,&#10;        'reconstruction': recon_data,&#10;        'diversity': diversity_data&#10;    }&#10;&#10;&#10;def plot_training_metrics(log_data: Dict, save_path: str = None):&#10;    &quot;&quot;&quot;绘制训练过程的综合指标图&quot;&quot;&quot;&#10;    fig, axes = plt.subplots(2, 3, figsize=(18, 12))&#10;    fig.suptitle('扩散模型训练过程分析', fontsize=16, fontweight='bold')&#10;&#10;    # 1. 损失曲线&#10;    ax1 = axes[0, 0]&#10;    ax1.plot(log_data['epochs'], log_data['losses'], 'b-', linewidth=2, alpha=0.8)&#10;    ax1.set_xlabel('Epoch')&#10;    ax1.set_ylabel('Loss')&#10;    ax1.set_title('训练损失曲线')&#10;    ax1.grid(True, alpha=0.3)&#10;&#10;    # 添加最佳损失点&#10;    min_loss_idx = np.argmin(log_data['losses'])&#10;    ax1.scatter(log_data['epochs'][min_loss_idx], log_data['losses'][min_loss_idx],&#10;                color='red', s=100, zorder=5, label=f'最佳损失: {log_data[&quot;losses&quot;][min_loss_idx]:.4f}')&#10;    ax1.legend()&#10;&#10;    # 2. 学习率调度&#10;    ax2 = axes[0, 1]&#10;    ax2.semilogy(log_data['epochs'], log_data['learning_rates'], 'g-', linewidth=2)&#10;    ax2.set_xlabel('Epoch')&#10;    ax2.set_ylabel('Learning Rate (log scale)')&#10;    ax2.set_title('学习率调度')&#10;    ax2.grid(True, alpha=0.3)&#10;&#10;    # 3. 重构准确率随时间步变化&#10;    ax3 = axes[0, 2]&#10;    recon_data = log_data['reconstruction']&#10;    if recon_data['epoch']:&#10;        for t_step in ['20', '50', '100', '200']:&#10;            if recon_data[f't{t_step}']:&#10;                ax3.plot(recon_data['epoch'], recon_data[f't{t_step}'],&#10;                        marker='o', label=f't={t_step}', linewidth=2, markersize=4)&#10;    ax3.set_xlabel('Epoch')&#10;    ax3.set_ylabel('重构准确率')&#10;    ax3.set_title('不同时间步的重构准确率')&#10;    ax3.legend()&#10;    ax3.grid(True, alpha=0.3)&#10;&#10;    # 4. 生成多样性&#10;    ax4 = axes[1, 0]&#10;    div_data = log_data['diversity']&#10;    if div_data['epoch']:&#10;        ax4.plot(div_data['epoch'], div_data['diversity_score'], 'purple',&#10;                marker='s', linewidth=2, markersize=6, label='多样性分数')&#10;        ax4.set_xlabel('Epoch')&#10;        ax4.set_ylabel('多样性分数')&#10;        ax4.set_title('生成序列多样性')&#10;        ax4.set_ylim(0, 1.1)&#10;        ax4.grid(True, alpha=0.3)&#10;        ax4.legend()&#10;&#10;    # 5. 有效序列生成率&#10;    ax5 = axes[1, 1]&#10;    if div_data['epoch']:&#10;        valid_ratios = [v/t for v, t in zip(div_data['valid_sequences'], div_data['total_sequences'])]&#10;        ax5.plot(div_data['epoch'], valid_ratios, 'orange',&#10;                marker='^', linewidth=2, markersize=6, label='有效序列率')&#10;        ax5.set_xlabel('Epoch')&#10;        ax5.set_ylabel('有效序列比例')&#10;        ax5.set_title('有效序列生成率')&#10;        ax5.set_ylim(0, 1.1)&#10;        ax5.grid(True, alpha=0.3)&#10;        ax5.legend()&#10;&#10;    # 6. 生成序列平均长度&#10;    ax6 = axes[1, 2]&#10;    if div_data['epoch']:&#10;        ax6.plot(div_data['epoch'], div_data['avg_length'], 'brown',&#10;                marker='d', linewidth=2, markersize=6, label='平均长度')&#10;        ax6.set_xlabel('Epoch')&#10;        ax6.set_ylabel('平均序列长度')&#10;        ax6.set_title('生成序列平均长度变化')&#10;        ax6.grid(True, alpha=0.3)&#10;        ax6.legend()&#10;&#10;    plt.tight_layout()&#10;&#10;    if save_path:&#10;        plt.savefig(save_path, dpi=300, bbox_inches='tight')&#10;        print(f&quot;图表已保存到: {save_path}&quot;)&#10;&#10;    plt.show()&#10;&#10;&#10;def plot_reconstruction_heatmap(log_data: Dict, save_path: str = None):&#10;    &quot;&quot;&quot;绘制重构准确率热力图&quot;&quot;&quot;&#10;    recon_data = log_data['reconstruction']&#10;&#10;    if not recon_data['epoch']:&#10;        print(&quot;没有重构准确率数据&quot;)&#10;        return&#10;&#10;    # 准备热力图数据&#10;    epochs = recon_data['epoch']&#10;    timesteps = ['t=20', 't=50', 't=100', 't=200']&#10;&#10;    # 创建数据矩阵&#10;    data_matrix = []&#10;    for t_step in ['20', '50', '100', '200']:&#10;        data_matrix.append(recon_data[f't{t_step}'])&#10;&#10;    data_matrix = np.array(data_matrix)&#10;&#10;    plt.figure(figsize=(12, 6))&#10;    sns.heatmap(data_matrix,&#10;                xticklabels=epochs,&#10;                yticklabels=timesteps,&#10;                annot=True,&#10;                fmt='.3f',&#10;                cmap='YlOrRd',&#10;                cbar_kws={'label': '重构准确率'})&#10;&#10;    plt.title('不同时间步重构准确率热力图', fontsize=14, fontweight='bold')&#10;    plt.xlabel('Epoch', fontsize=12)&#10;    plt.ylabel('扩散时间步', fontsize=12)&#10;    plt.xticks(rotation=45)&#10;&#10;    if save_path:&#10;        plt.savefig(save_path, dpi=300, bbox_inches='tight')&#10;        print(f&quot;热力图已保存到: {save_path}&quot;)&#10;&#10;    plt.tight_layout()&#10;    plt.show()&#10;&#10;&#10;def plot_early_stopping_analysis(log_data: Dict, save_path: str = None):&#10;    &quot;&quot;&quot;绘制早停机制分析图&quot;&quot;&quot;&#10;    patience_data = log_data['patience_counts']&#10;&#10;    if not patience_data:&#10;        print(&quot;没有早停数据&quot;)&#10;        return&#10;&#10;    epochs_with_patience = [item[0] for item in patience_data]&#10;    patience_values = [item[1] for item in patience_data]&#10;&#10;    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))&#10;&#10;    # 上图：损失曲线 + 早停标记&#10;    ax1.plot(log_data['epochs'], log_data['losses'], 'b-', linewidth=2, alpha=0.7, label='训练损失')&#10;&#10;    # 标记早停耐心点&#10;    for epoch, patience in patience_data:&#10;        if patience &gt; 0:  # 只显示有耐心计数的点&#10;            loss_idx = log_data['epochs'].index(epoch) if epoch in log_data['epochs'] else None&#10;            if loss_idx is not None:&#10;                ax1.scatter(epoch, log_data['losses'][loss_idx],&#10;                           color='red', s=50, alpha=0.6, zorder=5)&#10;&#10;    ax1.set_xlabel('Epoch')&#10;    ax1.set_ylabel('Loss')&#10;    ax1.set_title('训练损失曲线与早停标记')&#10;    ax1.legend()&#10;    ax1.grid(True, alpha=0.3)&#10;&#10;    # 下图：耐心计数变化&#10;    ax2.plot(epochs_with_patience, patience_values, 'ro-', linewidth=2, markersize=6)&#10;    ax2.fill_between(epochs_with_patience, patience_values, alpha=0.3, color='red')&#10;    ax2.axhline(y=30, color='orange', linestyle='--', linewidth=2, label='早停阈值 (30)')&#10;    ax2.set_xlabel('Epoch')&#10;    ax2.set_ylabel('耐心计数')&#10;    ax2.set_title('早停耐心计数变化')&#10;    ax2.legend()&#10;    ax2.grid(True, alpha=0.3)&#10;&#10;    plt.tight_layout()&#10;&#10;    if save_path:&#10;        plt.savefig(save_path, dpi=300, bbox_inches='tight')&#10;        print(f&quot;早停分析图已保存到: {save_path}&quot;)&#10;&#10;    plt.show()&#10;&#10;&#10;def generate_training_summary_report(log_data: Dict, save_path: str = None):&#10;    &quot;&quot;&quot;生成训练总结报告&quot;&quot;&quot;&#10;    report = []&#10;    report.append(&quot;=&quot; * 60)&#10;    report.append(&quot;扩散模型训练总结报告&quot;)&#10;    report.append(&quot;=&quot; * 60)&#10;&#10;    # 基础训练信息&#10;    total_epochs = len(log_data['epochs'])&#10;    final_loss = log_data['losses'][-1]&#10;    best_loss = min(log_data['losses'])&#10;    best_epoch = log_data['epochs'][log_data['losses'].index(best_loss)]&#10;&#10;    report.append(f&quot;\n【训练基础信息】&quot;)&#10;    report.append(f&quot;总训练轮数: {total_epochs}&quot;)&#10;    report.append(f&quot;最终损失: {final_loss:.4f}&quot;)&#10;    report.append(f&quot;最佳损失: {best_loss:.4f} (第{best_epoch}轮)&quot;)&#10;    report.append(f&quot;损失改善: {((log_data['losses'][0] - best_loss) / log_data['losses'][0] * 100):.2f}%&quot;)&#10;&#10;    # 重构准确率分析&#10;    recon_data = log_data['reconstruction']&#10;    if recon_data['epoch']:&#10;        report.append(f&quot;\n【重构准确率分析】&quot;)&#10;        for t_step in ['20', '50', '100', '200']:&#10;            if recon_data[f't{t_step}']:&#10;                avg_acc = np.mean(recon_data[f't{t_step}'])&#10;                final_acc = recon_data[f't{t_step}'][-1]&#10;                report.append(f&quot;t={t_step}: 平均={avg_acc:.4f}, 最终={final_acc:.4f}&quot;)&#10;&#10;    # 生成质量分析&#10;    div_data = log_data['diversity']&#10;    if div_data['epoch']:&#10;        report.append(f&quot;\n【生成质量分析】&quot;)&#10;        avg_diversity = np.mean(div_data['diversity_score'])&#10;        avg_valid_ratio = np.mean([v/t for v, t in zip(div_data['valid_sequences'], div_data['total_sequences'])])&#10;        avg_length = np.mean(div_data['avg_length'])&#10;&#10;        report.append(f&quot;平均多样性分数: {avg_diversity:.4f}&quot;)&#10;        report.append(f&quot;平均有效序列率: {avg_valid_ratio:.4f}&quot;)&#10;        report.append(f&quot;平均序列长度: {avg_length:.1f}&quot;)&#10;&#10;    # 早停分析&#10;    if log_data['patience_counts']:&#10;        max_patience = max([item[1] for item in log_data['patience_counts']])&#10;        report.append(f&quot;\n【早停机制分析】&quot;)&#10;        report.append(f&quot;最大耐心计数: {max_patience}/30&quot;)&#10;        report.append(f&quot;是否触发早停: {'是' if total_epochs &lt; 200 else '否'}&quot;)&#10;&#10;    report.append(&quot;\n&quot; + &quot;=&quot; * 60)&#10;&#10;    # 保存报告&#10;    if save_path:&#10;        with open(save_path, 'w', encoding='utf-8') as f:&#10;            f.write('\n'.join(report))&#10;        print(f&quot;训练总结报告已保存到: {save_path}&quot;)&#10;&#10;    # 打印报告&#10;    for line in report:&#10;        print(line)&#10;&#10;&#10;# 示例使用函数&#10;def analyze_training_from_log(log_file_path: str, output_dir: str = &quot;./analysis_results/&quot;):&#10;    &quot;&quot;&quot;从日志文件分析训练过程并生成所有图表&quot;&quot;&quot;&#10;    import os&#10;&#10;    # 创建输出目录&#10;    os.makedirs(output_dir, exist_ok=True)&#10;&#10;    # 解析日志&#10;    print(&quot;正在解析训练日志...&quot;)&#10;    log_data = parse_training_log(log_file_path)&#10;&#10;    # 生成各种图表&#10;    print(&quot;正在生成训练指标图...&quot;)&#10;    plot_training_metrics(log_data, f&quot;{output_dir}/training_metrics.png&quot;)&#10;&#10;    print(&quot;正在生成重构准确率热力图...&quot;)&#10;    plot_reconstruction_heatmap(log_data, f&quot;{output_dir}/reconstruction_heatmap.png&quot;)&#10;&#10;    print(&quot;正在生成早停分析图...&quot;)&#10;    plot_early_stopping_analysis(log_data, f&quot;{output_dir}/early_stopping_analysis.png&quot;)&#10;&#10;    print(&quot;正在生成训练总结报告...&quot;)&#10;    generate_training_summary_report(log_data, f&quot;{output_dir}/training_summary.txt&quot;)&#10;&#10;    print(f&quot;所有分析结果已保存到: {output_dir}&quot;)&#10;&#10;    return log_data&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # 示例1: 分析AMP长度分布&#10;    try:&#10;        plot_amp_length_distribution(&quot;./classify_data/classify.csv&quot;)&#10;    except Exception as e:&#10;        print(f&quot;绘制AMP长度分布时出错: {e}&quot;)&#10;&#10;    # 示例2: 从日志文件分析训练过程&#10;    # analyze_training_from_log(&quot;training.log&quot;, &quot;./analysis_results/&quot;)&#10;&#10;    # 如果您有训练日志文件，请使用以下代码：&#10;    print(&quot;\n要分析训练日志，请使用:&quot;)&#10;    print(&quot;analyze_training_from_log('your_training_log.txt', './analysis_results/')&quot;)&#10;" />
              <option name="updatedContent" value="import pandas as pd&#10;import matplotlib.pyplot as plt&#10;import numpy as np&#10;import seaborn as sns&#10;from typing import List, Dict, Tuple&#10;import re&#10;&#10;# Set font and plot style&#10;plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']&#10;plt.rcParams['axes.unicode_minus'] = False&#10;sns.set_style(&quot;whitegrid&quot;)&#10;plt.rcParams['figure.dpi'] = 300&#10;&#10;&#10;def plot_amp_length_distribution(csv_path):&#10;    &quot;&quot;&quot;Plot AMP length distribution&quot;&quot;&quot;&#10;    df = pd.read_csv(csv_path)&#10;    df_pos = df[df['label'] == 1]&#10;&#10;    if 'sequence' not in df_pos.columns:&#10;        raise ValueError(&quot;CSV file missing 'sequence' column&quot;)&#10;&#10;    lengths = df_pos['sequence'].apply(len)&#10;    lengths = lengths[(lengths &gt;= 6) &amp; (lengths &lt;= 75)]&#10;    length_counts = lengths.value_counts().sort_index()&#10;&#10;    plt.figure(figsize=(12, 6))&#10;    plt.bar(length_counts.index, length_counts.values, color='skyblue', alpha=0.7, edgecolor='navy')&#10;    plt.xlabel('Peptide Length (amino acids)', fontsize=12)&#10;    plt.ylabel('Count', fontsize=12)&#10;    plt.title('Positive Sample AMP Length Distribution', fontsize=14, fontweight='bold')&#10;    plt.grid(True, alpha=0.3)&#10;    plt.tight_layout()&#10;    plt.show()&#10;&#10;&#10;def parse_training_log_from_text(log_content: str) -&gt; Dict:&#10;    &quot;&quot;&quot;Parse key metrics from log text content&quot;&quot;&quot;&#10;    # Extract training data&#10;    epochs = []&#10;    losses = []&#10;    learning_rates = []&#10;    patience_counts = []&#10;&#10;    # Reconstruction accuracy data&#10;    recon_data = {&#10;        'epoch': [],&#10;        't20': [], 't50': [], 't100': [], 't200': []&#10;    }&#10;&#10;    # Diversity data&#10;    diversity_data = {&#10;        'epoch': [],&#10;        'valid_sequences': [],&#10;        'total_sequences': [],&#10;        'diversity_score': [],&#10;        'avg_length': []&#10;    }&#10;&#10;    lines = log_content.split('\n')&#10;    current_epoch = None&#10;&#10;    for line in lines:&#10;        # Extract epoch, loss, learning rate&#10;        epoch_match = re.search(r'Epoch (\d+) finished\. Avg Loss: ([\d.]+), LR: ([\d.e-]+)', line)&#10;        if epoch_match:&#10;            current_epoch = int(epoch_match.group(1))&#10;            epochs.append(current_epoch)&#10;            losses.append(float(epoch_match.group(2)))&#10;            learning_rates.append(float(epoch_match.group(3)))&#10;&#10;        # Extract patience count&#10;        patience_match = re.search(r'Patience: (\d+)/(\d+)', line)&#10;        if patience_match and current_epoch is not None:&#10;            patience_counts.append((current_epoch, int(patience_match.group(1))))&#10;&#10;        # Extract reconstruction accuracy&#10;        if 'Reconstruction Accuracy:' in line and current_epoch is not None:&#10;            recon_data['epoch'].append(current_epoch)&#10;&#10;        recon_match = re.search(r't=(\d+): ([\d.]+)', line)&#10;        if recon_match and len(recon_data['epoch']) &gt; 0 and current_epoch == recon_data['epoch'][-1]:&#10;            t_val = recon_match.group(1)&#10;            acc_val = float(recon_match.group(2))&#10;            if t_val in ['20', '50', '100', '200']:&#10;                # Ensure each timestep has corresponding data&#10;                while len(recon_data[f't{t_val}']) &lt; len(recon_data['epoch']):&#10;                    recon_data[f't{t_val}'].append(acc_val)&#10;&#10;        # Extract diversity data&#10;        diversity_match = re.search(r'Valid sequences: (\d+)/(\d+)', line)&#10;        if diversity_match and current_epoch is not None:&#10;            valid_seq = int(diversity_match.group(1))&#10;            total_seq = int(diversity_match.group(2))&#10;            diversity_data['epoch'].append(current_epoch)&#10;            diversity_data['valid_sequences'].append(valid_seq)&#10;            diversity_data['total_sequences'].append(total_seq)&#10;&#10;        diversity_score_match = re.search(r'Generation diversity: ([\d.]+), Avg length: ([\d.]+)', line)&#10;        if diversity_score_match and len(diversity_data['diversity_score']) &lt; len(diversity_data['epoch']):&#10;            diversity_data['diversity_score'].append(float(diversity_score_match.group(1)))&#10;            diversity_data['avg_length'].append(float(diversity_score_match.group(2)))&#10;&#10;    # Ensure reconstruction data consistency&#10;    for t_step in ['20', '50', '100', '200']:&#10;        while len(recon_data[f't{t_step}']) &lt; len(recon_data['epoch']):&#10;            recon_data[f't{t_step}'].append(0.0)  # Fill missing values&#10;&#10;    return {&#10;        'epochs': epochs,&#10;        'losses': losses,&#10;        'learning_rates': learning_rates,&#10;        'patience_counts': patience_counts,&#10;        'reconstruction': recon_data,&#10;        'diversity': diversity_data&#10;    }&#10;&#10;&#10;def parse_training_log(log_file_path: str) -&gt; Dict:&#10;    &quot;&quot;&quot;Parse training log file and extract key metrics&quot;&quot;&quot;&#10;    with open(log_file_path, 'r', encoding='utf-8') as f:&#10;        content = f.read()&#10;    return parse_training_log_from_text(content)&#10;&#10;&#10;def plot_training_metrics(log_data: Dict, save_path: str = None):&#10;    &quot;&quot;&quot;Plot comprehensive training metrics&quot;&quot;&quot;&#10;    fig, axes = plt.subplots(2, 3, figsize=(18, 12))&#10;    fig.suptitle('Diffusion Model Training Analysis', fontsize=16, fontweight='bold')&#10;&#10;    # 1. Loss curve&#10;    ax1 = axes[0, 0]&#10;    ax1.plot(log_data['epochs'], log_data['losses'], 'b-', linewidth=2, alpha=0.8)&#10;    ax1.set_xlabel('Epoch')&#10;    ax1.set_ylabel('Loss')&#10;    ax1.set_title('Training Loss Curve')&#10;    ax1.grid(True, alpha=0.3)&#10;&#10;    # Add best loss point&#10;    min_loss_idx = np.argmin(log_data['losses'])&#10;    ax1.scatter(log_data['epochs'][min_loss_idx], log_data['losses'][min_loss_idx],&#10;                color='red', s=100, zorder=5, label=f'Best Loss: {log_data[&quot;losses&quot;][min_loss_idx]:.4f}')&#10;    ax1.legend()&#10;&#10;    # 2. Learning rate schedule&#10;    ax2 = axes[0, 1]&#10;    ax2.semilogy(log_data['epochs'], log_data['learning_rates'], 'g-', linewidth=2)&#10;    ax2.set_xlabel('Epoch')&#10;    ax2.set_ylabel('Learning Rate (log scale)')&#10;    ax2.set_title('Learning Rate Schedule')&#10;    ax2.grid(True, alpha=0.3)&#10;&#10;    # 3. Reconstruction accuracy by timestep&#10;    ax3 = axes[0, 2]&#10;    recon_data = log_data['reconstruction']&#10;    if recon_data['epoch']:&#10;        for t_step in ['20', '50', '100', '200']:&#10;            if recon_data[f't{t_step}']:&#10;                ax3.plot(recon_data['epoch'], recon_data[f't{t_step}'],&#10;                        marker='o', label=f't={t_step}', linewidth=2, markersize=4)&#10;    ax3.set_xlabel('Epoch')&#10;    ax3.set_ylabel('Reconstruction Accuracy')&#10;    ax3.set_title('Reconstruction Accuracy by Timestep')&#10;    ax3.legend()&#10;    ax3.grid(True, alpha=0.3)&#10;&#10;    # 4. Generation diversity&#10;    ax4 = axes[1, 0]&#10;    div_data = log_data['diversity']&#10;    if div_data['epoch']:&#10;        ax4.plot(div_data['epoch'], div_data['diversity_score'], 'purple',&#10;                marker='s', linewidth=2, markersize=6, label='Diversity Score')&#10;        ax4.set_xlabel('Epoch')&#10;        ax4.set_ylabel('Diversity Score')&#10;        ax4.set_title('Generation Diversity')&#10;        ax4.set_ylim(0, 1.1)&#10;        ax4.grid(True, alpha=0.3)&#10;        ax4.legend()&#10;&#10;    # 5. Valid sequence generation rate&#10;    ax5 = axes[1, 1]&#10;    if div_data['epoch']:&#10;        valid_ratios = [v/t for v, t in zip(div_data['valid_sequences'], div_data['total_sequences'])]&#10;        ax5.plot(div_data['epoch'], valid_ratios, 'orange',&#10;                marker='^', linewidth=2, markersize=6, label='Valid Sequence Rate')&#10;        ax5.set_xlabel('Epoch')&#10;        ax5.set_ylabel('Valid Sequence Ratio')&#10;        ax5.set_title('Valid Sequence Generation Rate')&#10;        ax5.set_ylim(0, 1.1)&#10;        ax5.grid(True, alpha=0.3)&#10;        ax5.legend()&#10;&#10;    # 6. Average sequence length&#10;    ax6 = axes[1, 2]&#10;    if div_data['epoch']:&#10;        ax6.plot(div_data['epoch'], div_data['avg_length'], 'brown',&#10;                marker='d', linewidth=2, markersize=6, label='Average Length')&#10;        ax6.set_xlabel('Epoch')&#10;        ax6.set_ylabel('Average Sequence Length')&#10;        ax6.set_title('Generated Sequence Average Length')&#10;        ax6.grid(True, alpha=0.3)&#10;        ax6.legend()&#10;&#10;    plt.tight_layout()&#10;&#10;    if save_path:&#10;        plt.savefig(save_path, dpi=300, bbox_inches='tight')&#10;        print(f&quot;Figure saved to: {save_path}&quot;)&#10;&#10;    plt.show()&#10;&#10;&#10;def plot_reconstruction_heatmap(log_data: Dict, save_path: str = None):&#10;    &quot;&quot;&quot;Plot reconstruction accuracy heatmap&quot;&quot;&quot;&#10;    recon_data = log_data['reconstruction']&#10;&#10;    if not recon_data['epoch']:&#10;        print(&quot;No reconstruction accuracy data&quot;)&#10;        return&#10;&#10;    # Prepare heatmap data&#10;    epochs = recon_data['epoch']&#10;    timesteps = ['t=20', 't=50', 't=100', 't=200']&#10;&#10;    # Create data matrix&#10;    data_matrix = []&#10;    for t_step in ['20', '50', '100', '200']:&#10;        data_matrix.append(recon_data[f't{t_step}'])&#10;&#10;    data_matrix = np.array(data_matrix)&#10;&#10;    plt.figure(figsize=(12, 6))&#10;    sns.heatmap(data_matrix,&#10;                xticklabels=epochs,&#10;                yticklabels=timesteps,&#10;                annot=True,&#10;                fmt='.3f',&#10;                cmap='YlOrRd',&#10;                cbar_kws={'label': 'Reconstruction Accuracy'})&#10;&#10;    plt.title('Reconstruction Accuracy Heatmap by Timestep', fontsize=14, fontweight='bold')&#10;    plt.xlabel('Epoch', fontsize=12)&#10;    plt.ylabel('Diffusion Timestep', fontsize=12)&#10;    plt.xticks(rotation=45)&#10;&#10;    if save_path:&#10;        plt.savefig(save_path, dpi=300, bbox_inches='tight')&#10;        print(f&quot;Heatmap saved to: {save_path}&quot;)&#10;&#10;    plt.tight_layout()&#10;    plt.show()&#10;&#10;&#10;def plot_early_stopping_analysis(log_data: Dict, save_path: str = None):&#10;    &quot;&quot;&quot;Plot early stopping mechanism analysis&quot;&quot;&quot;&#10;    patience_data = log_data['patience_counts']&#10;&#10;    if not patience_data:&#10;        print(&quot;No early stopping data&quot;)&#10;        return&#10;&#10;    epochs_with_patience = [item[0] for item in patience_data]&#10;    patience_values = [item[1] for item in patience_data]&#10;&#10;    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))&#10;&#10;    # Top plot: Loss curve with early stopping markers&#10;    ax1.plot(log_data['epochs'], log_data['losses'], 'b-', linewidth=2, alpha=0.7, label='Training Loss')&#10;&#10;    # Mark early stopping patience points&#10;    for epoch, patience in patience_data:&#10;        if patience &gt; 0:  # Only show points with patience count&#10;            loss_idx = log_data['epochs'].index(epoch) if epoch in log_data['epochs'] else None&#10;            if loss_idx is not None:&#10;                ax1.scatter(epoch, log_data['losses'][loss_idx],&#10;                           color='red', s=50, alpha=0.6, zorder=5)&#10;&#10;    ax1.set_xlabel('Epoch')&#10;    ax1.set_ylabel('Loss')&#10;    ax1.set_title('Training Loss Curve with Early Stopping Markers')&#10;    ax1.legend()&#10;    ax1.grid(True, alpha=0.3)&#10;&#10;    # Bottom plot: Patience count changes&#10;    ax2.plot(epochs_with_patience, patience_values, 'ro-', linewidth=2, markersize=6)&#10;    ax2.fill_between(epochs_with_patience, patience_values, alpha=0.3, color='red')&#10;    ax2.axhline(y=30, color='orange', linestyle='--', linewidth=2, label='Early Stop Threshold (30)')&#10;    ax2.set_xlabel('Epoch')&#10;    ax2.set_ylabel('Patience Count')&#10;    ax2.set_title('Early Stopping Patience Count')&#10;    ax2.legend()&#10;    ax2.grid(True, alpha=0.3)&#10;&#10;    plt.tight_layout()&#10;&#10;    if save_path:&#10;        plt.savefig(save_path, dpi=300, bbox_inches='tight')&#10;        print(f&quot;Early stopping analysis saved to: {save_path}&quot;)&#10;&#10;    plt.show()&#10;&#10;&#10;def generate_training_summary_report(log_data: Dict, save_path: str = None):&#10;    &quot;&quot;&quot;Generate training summary report&quot;&quot;&quot;&#10;    report = []&#10;    report.append(&quot;=&quot; * 60)&#10;    report.append(&quot;Diffusion Model Training Summary Report&quot;)&#10;    report.append(&quot;=&quot; * 60)&#10;&#10;    # Basic training information&#10;    total_epochs = len(log_data['epochs'])&#10;    final_loss = log_data['losses'][-1]&#10;    best_loss = min(log_data['losses'])&#10;    best_epoch = log_data['epochs'][log_data['losses'].index(best_loss)]&#10;&#10;    report.append(f&quot;\n[Basic Training Information]&quot;)&#10;    report.append(f&quot;Total training epochs: {total_epochs}&quot;)&#10;    report.append(f&quot;Final loss: {final_loss:.4f}&quot;)&#10;    report.append(f&quot;Best loss: {best_loss:.4f} (epoch {best_epoch})&quot;)&#10;    report.append(f&quot;Loss improvement: {((log_data['losses'][0] - best_loss) / log_data['losses'][0] * 100):.2f}%&quot;)&#10;&#10;    # Reconstruction accuracy analysis&#10;    recon_data = log_data['reconstruction']&#10;    if recon_data['epoch']:&#10;        report.append(f&quot;\n[Reconstruction Accuracy Analysis]&quot;)&#10;        for t_step in ['20', '50', '100', '200']:&#10;            if recon_data[f't{t_step}']:&#10;                avg_acc = np.mean(recon_data[f't{t_step}'])&#10;                final_acc = recon_data[f't{t_step}'][-1]&#10;                report.append(f&quot;t={t_step}: average={avg_acc:.4f}, final={final_acc:.4f}&quot;)&#10;&#10;    # Generation quality analysis&#10;    div_data = log_data['diversity']&#10;    if div_data['epoch']:&#10;        report.append(f&quot;\n[Generation Quality Analysis]&quot;)&#10;        avg_diversity = np.mean(div_data['diversity_score'])&#10;        avg_valid_ratio = np.mean([v/t for v, t in zip(div_data['valid_sequences'], div_data['total_sequences'])])&#10;        avg_length = np.mean(div_data['avg_length'])&#10;&#10;        report.append(f&quot;Average diversity score: {avg_diversity:.4f}&quot;)&#10;        report.append(f&quot;Average valid sequence rate: {avg_valid_ratio:.4f}&quot;)&#10;        report.append(f&quot;Average sequence length: {avg_length:.1f}&quot;)&#10;&#10;    # Early stopping analysis&#10;    if log_data['patience_counts']:&#10;        max_patience = max([item[1] for item in log_data['patience_counts']])&#10;        report.append(f&quot;\n[Early Stopping Analysis]&quot;)&#10;        report.append(f&quot;Maximum patience count: {max_patience}/30&quot;)&#10;        report.append(f&quot;Early stopping triggered: {'Yes' if total_epochs &lt; 200 else 'No'}&quot;)&#10;&#10;    report.append(&quot;\n&quot; + &quot;=&quot; * 60)&#10;&#10;    # Save report&#10;    if save_path:&#10;        with open(save_path, 'w', encoding='utf-8') as f:&#10;            f.write('\n'.join(report))&#10;        print(f&quot;Training summary report saved to: {save_path}&quot;)&#10;&#10;    # Print report&#10;    for line in report:&#10;        print(line)&#10;&#10;&#10;def analyze_training_from_log(log_file_path: str, output_dir: str = &quot;./analysis_results/&quot;):&#10;    &quot;&quot;&quot;Analyze training process from log file and generate all figures&quot;&quot;&quot;&#10;    import os&#10;&#10;    # Create output directory&#10;    os.makedirs(output_dir, exist_ok=True)&#10;&#10;    # Parse log&#10;    print(&quot;Parsing training log...&quot;)&#10;    log_data = parse_training_log(log_file_path)&#10;&#10;    # Generate various figures&#10;    print(&quot;Generating training metrics plot...&quot;)&#10;    plot_training_metrics(log_data, f&quot;{output_dir}/training_metrics.png&quot;)&#10;&#10;    print(&quot;Generating reconstruction accuracy heatmap...&quot;)&#10;    plot_reconstruction_heatmap(log_data, f&quot;{output_dir}/reconstruction_heatmap.png&quot;)&#10;&#10;    print(&quot;Generating early stopping analysis...&quot;)&#10;    plot_early_stopping_analysis(log_data, f&quot;{output_dir}/early_stopping_analysis.png&quot;)&#10;&#10;    print(&quot;Generating training summary report...&quot;)&#10;    generate_training_summary_report(log_data, f&quot;{output_dir}/training_summary.txt&quot;)&#10;&#10;    print(f&quot;All analysis results saved to: {output_dir}&quot;)&#10;&#10;    return log_data&#10;&#10;&#10;def analyze_training_from_text(log_content: str, output_dir: str = &quot;./analysis_results/&quot;):&#10;    &quot;&quot;&quot;Analyze training process from log text and generate all figures&quot;&quot;&quot;&#10;    import os&#10;&#10;    # Create output directory&#10;    os.makedirs(output_dir, exist_ok=True)&#10;&#10;    # Parse log&#10;    print(&quot;Parsing training log...&quot;)&#10;    log_data = parse_training_log_from_text(log_content)&#10;&#10;    # Generate various figures&#10;    print(&quot;Generating training metrics plot...&quot;)&#10;    plot_training_metrics(log_data, f&quot;{output_dir}/training_metrics.png&quot;)&#10;&#10;    print(&quot;Generating reconstruction accuracy heatmap...&quot;)&#10;    plot_reconstruction_heatmap(log_data, f&quot;{output_dir}/reconstruction_heatmap.png&quot;)&#10;&#10;    print(&quot;Generating early stopping analysis...&quot;)&#10;    plot_early_stopping_analysis(log_data, f&quot;{output_dir}/early_stopping_analysis.png&quot;)&#10;&#10;    print(&quot;Generating training summary report...&quot;)&#10;    generate_training_summary_report(log_data, f&quot;{output_dir}/training_summary.txt&quot;)&#10;&#10;    print(f&quot;All analysis results saved to: {output_dir}&quot;)&#10;&#10;    return log_data&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # Example 1: Analyze AMP length distribution&#10;    try:&#10;        plot_amp_length_distribution(&quot;./classify_data/classify.csv&quot;)&#10;    except Exception as e:&#10;        print(f&quot;Error plotting AMP length distribution: {e}&quot;)&#10;&#10;    # Example 2: Analyze training process from log file&#10;    # analyze_training_from_log(&quot;training.log&quot;, &quot;./analysis_results/&quot;)&#10;&#10;    # If you have a training log file, use:&#10;    print(&quot;\nTo analyze training log, use:&quot;)&#10;    print(&quot;analyze_training_from_log('your_training_log.txt', './analysis_results/')&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/calculate_vespa_properties.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/calculate_vespa_properties.py" />
              <option name="updatedContent" value="import pandas as pd&#10;from Bio.SeqUtils.ProtParam import ProteinAnalysis&#10;import os&#10;import argparse&#10;&#10;class VespaPropertiesCalculator:&#10;    def __init__(self):&#10;        self.hydrophobic_aa = set(['A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W'])&#10;        self.hydrophilic_aa = set(['R', 'K', 'D', 'E', 'Q', 'N', 'H', 'S', 'T', 'G', 'P', 'C'])&#10;        self.positively_charged_aa = set(['R', 'K', 'H'])&#10;        self.negatively_charged_aa = set(['D', 'E'])&#10;&#10;    def calculate_amphipathicity(self, sequence):&#10;        &quot;&quot;&quot;计算两亲性指数&quot;&quot;&quot;&#10;        hydrophobic_count = sum(1 for aa in sequence if aa in self.hydrophobic_aa)&#10;        hydrophilic_count = sum(1 for aa in sequence if aa in self.hydrophilic_aa)&#10;        total_count = len(sequence)&#10;        if total_count == 0:&#10;            return 0&#10;        hydrophobic_ratio = hydrophobic_count / total_count&#10;        hydrophilic_ratio = hydrophilic_count / total_count&#10;        if hydrophobic_ratio + hydrophilic_ratio == 0:&#10;            return 0&#10;        amphipathicity = 2 * hydrophobic_ratio * hydrophilic_ratio / (hydrophobic_ratio + hydrophilic_ratio)&#10;        return amphipathicity&#10;&#10;    def calculate_net_charge_at_ph(self, sequence, ph=7.4):&#10;        &quot;&quot;&quot;计算在指定pH下的净电荷&quot;&quot;&quot;&#10;        try:&#10;            protein_analysis = ProteinAnalysis(sequence)&#10;            net_charge = protein_analysis.charge_at_pH(ph)&#10;            return net_charge&#10;        except Exception as e:&#10;            print(f&quot;Error calculating charge for sequence {sequence}: {e}&quot;)&#10;            return 0&#10;&#10;    def calculate_additional_properties(self, sequence):&#10;        &quot;&quot;&quot;计算额外的理化性质&quot;&quot;&quot;&#10;        try:&#10;            protein_analysis = ProteinAnalysis(sequence)&#10;            properties = {&#10;                'molecular_weight': protein_analysis.molecular_weight(),&#10;                'aromaticity': protein_analysis.aromaticity(),&#10;                'instability_index': protein_analysis.instability_index(),&#10;                'isoelectric_point': protein_analysis.isoelectric_point(),&#10;                'gravy': protein_analysis.gravy(),&#10;                'helix_fraction': protein_analysis.secondary_structure_fraction()[0],&#10;                'turn_fraction': protein_analysis.secondary_structure_fraction()[1],&#10;                'sheet_fraction': protein_analysis.secondary_structure_fraction()[2]&#10;            }&#10;            return properties&#10;        except Exception as e:&#10;            print(f&quot;Error calculating properties for sequence {sequence}: {e}&quot;)&#10;            return {&#10;                'molecular_weight': 0,&#10;                'aromaticity': 0,&#10;                'instability_index': 0,&#10;                'isoelectric_point': 0,&#10;                'gravy': 0,&#10;                'helix_fraction': 0,&#10;                'turn_fraction': 0,&#10;                'sheet_fraction': 0&#10;            }&#10;&#10;    def calculate_all_properties(self, input_file, output_file=None, ph=7.4):&#10;        &quot;&quot;&quot;计算所有序列的理化性质，不进行筛选&quot;&quot;&quot;&#10;        if not os.path.exists(input_file):&#10;            print(f&quot;输入文件 {input_file} 不存在&quot;)&#10;            return None&#10;&#10;        # 读取CSV文件，使用分号作为分隔符&#10;        df = pd.read_csv(input_file, sep=';')&#10;        &#10;        if 'sequence' not in df.columns:&#10;            print(&quot;输入文件中没有找到序列列，请确保列名为'sequence'&quot;)&#10;            return None&#10;&#10;        print(f&quot;开始计算，共有 {len(df)} 个序列&quot;)&#10;&#10;        results = []&#10;&#10;        for idx, row in df.iterrows():&#10;            sequence = str(row['sequence']).strip().upper()&#10;            if not sequence or not sequence.isalpha():&#10;                print(f&quot;跳过无效序列: {sequence}&quot;)&#10;                continue&#10;&#10;            print(f&quot;正在处理序列 {idx+1}/{len(df)}: {sequence}&quot;)&#10;&#10;            # 计算基本属性&#10;            length = len(sequence)&#10;            amphipathicity = self.calculate_amphipathicity(sequence)&#10;            net_charge = self.calculate_net_charge_at_ph(sequence, ph)&#10;            &#10;            # 计算额外理化性质&#10;            additional_props = self.calculate_additional_properties(sequence)&#10;            &#10;            # 构建结果字典&#10;            result = {&#10;                'sequence': sequence,&#10;                'length': length,&#10;                'amphipathicity': amphipathicity,&#10;                'net_charge_ph7.4': net_charge,&#10;            }&#10;            &#10;            # 添加原始数据的其他列&#10;            for col in df.columns:&#10;                if col != 'sequence' and col not in result:&#10;                    result[col] = row[col]&#10;            &#10;            # 添加计算的理化性质&#10;            result.update(additional_props)&#10;            &#10;            results.append(result)&#10;&#10;        if results:&#10;            results_df = pd.DataFrame(results)&#10;            print(f&quot;\n计算完成！共处理了 {len(results_df)} 个有效序列&quot;)&#10;            &#10;            # 打印统计信息&#10;            print(&quot;\n计算结果统计:&quot;)&#10;            print(f&quot;序列长度范围: {results_df['length'].min()}-{results_df['length'].max()}&quot;)&#10;            print(f&quot;两亲性指数范围: {results_df['amphipathicity'].min():.3f}-{results_df['amphipathicity'].max():.3f}&quot;)&#10;            print(f&quot;净电荷范围: {results_df['net_charge_ph7.4'].min():.3f}-{results_df['net_charge_ph7.4'].max():.3f}&quot;)&#10;            print(f&quot;分子量范围: {results_df['molecular_weight'].min():.1f}-{results_df['molecular_weight'].max():.1f}&quot;)&#10;            print(f&quot;等电点范围: {results_df['isoelectric_point'].min():.2f}-{results_df['isoelectric_point'].max():.2f}&quot;)&#10;            print(f&quot;GRAVY值范围: {results_df['gravy'].min():.3f}-{results_df['gravy'].max():.3f}&quot;)&#10;            &#10;            if output_file:&#10;                results_df.to_csv(output_file, index=False)&#10;                print(f&quot;\n计算结果已保存到: {output_file}&quot;)&#10;            &#10;            return results_df&#10;        else:&#10;            print(&quot;没有有效的序列进行计算&quot;)&#10;            return pd.DataFrame()&#10;&#10;def main():&#10;    parser = argparse.ArgumentParser(description='计算Vespa数据集中所有肽段的理化性质')&#10;    parser.add_argument('--input', help='输入的Vespa CSV文件路径',&#10;                        default='origin_data/Vespa.csv')&#10;    parser.add_argument('--output', help='输出文件路径',&#10;                       default='results/vespa_properties.csv')&#10;    parser.add_argument('--ph', type=float, default=7.4,&#10;                       help='计算净电荷的pH值 (默认: 7.4)')&#10;&#10;    args = parser.parse_args()&#10;    &#10;    # 确保输出目录存在&#10;    os.makedirs(os.path.dirname(args.output), exist_ok=True)&#10;    &#10;    calculator = VespaPropertiesCalculator()&#10;    results_df = calculator.calculate_all_properties(&#10;        input_file=args.input,&#10;        output_file=args.output,&#10;        ph=args.ph&#10;    )&#10;    &#10;    if results_df is not None and len(results_df) &gt; 0:&#10;        print(f&quot;\n计算成功完成！共计算了 {len(results_df)} 个序列的理化性质&quot;)&#10;        print(f&quot;结果已保存到: {args.output}&quot;)&#10;    else:&#10;        print(&quot;计算失败或没有有效的序列&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/experience_filter_improved.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/experience_filter_improved.py" />
              <option name="updatedContent" value="#!/usr/bin/env python3&#10;# -*- coding: utf-8 -*-&#10;&quot;&quot;&quot;&#10;改进的经验筛选脚本 - 使用Biopython进行更准确的计算&#10;筛选条件：&#10;1. 具有两亲性（疏水性和亲水性氨基酸的平衡）&#10;2. 序列长度为15-17个氨基酸&#10;3. 在生理pH值下净正电荷&#10;&#10;Author: GitHub Copilot&#10;Date: 2025-01-04&#10;&quot;&quot;&quot;&#10;&#10;import pandas as pd&#10;import numpy as np&#10;import os&#10;import argparse&#10;&#10;try:&#10;    from Bio.SeqUtils.ProtParam import ProteinAnalysis&#10;    from Bio.SeqUtils import molecular_weight&#10;    from Bio.Seq import Seq&#10;    BIOPYTHON_AVAILABLE = True&#10;except ImportError:&#10;    BIOPYTHON_AVAILABLE = False&#10;    print(&quot;警告: Biopython未安装，将使用简化计算方法&quot;)&#10;&#10;&#10;class ImprovedExperienceFilter:&#10;    def __init__(self):&#10;        # Kyte-Doolittle疏水性指数（标准化）&#10;        self.kyte_doolittle = {&#10;            'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,&#10;            'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,&#10;            'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,&#10;            'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2&#10;        }&#10;        &#10;        # 氨基酸电荷（在pH 7.0时）&#10;        self.aa_charges_ph7 = {&#10;            'R': 1.0, 'K': 1.0, 'H': 0.1, 'D': -1.0, 'E': -1.0,&#10;            'C': 0.0, 'Y': 0.0  # 其他氨基酸在生理pH下基本不带电&#10;        }&#10;        &#10;        # 氨基酸分类&#10;        self.hydrophobic_aa = set(['A', 'I', 'L', 'M', 'F', 'W', 'Y', 'V', 'C'])&#10;        self.hydrophilic_aa = set(['R', 'N', 'D', 'Q', 'E', 'H', 'K', 'S', 'T'])&#10;        self.charged_aa = set(['R', 'K', 'H', 'D', 'E'])&#10;        self.polar_aa = set(['N', 'D', 'Q', 'E', 'H', 'K', 'S', 'T', 'R', 'Y', 'C'])&#10;    &#10;    def calculate_length_filter(self, sequence):&#10;        &quot;&quot;&quot;检查序列长度&quot;&quot;&quot;&#10;        return 15 &lt;= len(sequence) &lt;= 17&#10;    &#10;    def calculate_net_charge_biopython(self, sequence, ph=7.4):&#10;        &quot;&quot;&quot;使用Biopython计算净电荷&quot;&quot;&quot;&#10;        if not BIOPYTHON_AVAILABLE:&#10;            return self.calculate_net_charge_simple(sequence, ph)&#10;        &#10;        try:&#10;            protein_analysis = ProteinAnalysis(str(sequence))&#10;            return protein_analysis.charge_at_pH(ph)&#10;        except Exception:&#10;            return self.calculate_net_charge_simple(sequence, ph)&#10;    &#10;    def calculate_net_charge_simple(self, sequence, ph=7.4):&#10;        &quot;&quot;&quot;简化的净电荷计算（备用方法）&quot;&quot;&quot;&#10;        net_charge = 0&#10;        for aa in sequence:&#10;            if aa == 'R':&#10;                net_charge += 1.0  # 精氨酸在生理pH下完全质子化&#10;            elif aa == 'K':&#10;                net_charge += 1.0  # 赖氨酸在生理pH下完全质子化&#10;            elif aa == 'H':&#10;                # 组氨酸的pKa约为6.0，在pH 7.4时部分质子化&#10;                net_charge += 1 / (1 + 10**(ph - 6.0))&#10;            elif aa == 'D':&#10;                net_charge -= 1.0  # 天冬氨酸在生理pH下完全去质子化&#10;            elif aa == 'E':&#10;                net_charge -= 1.0  # 谷氨酸在生理pH下完全去质子化&#10;        return net_charge&#10;    &#10;    def calculate_hydrophobicity_biopython(self, sequence):&#10;        &quot;&quot;&quot;使用Biopython计算疏水性&quot;&quot;&quot;&#10;        if not BIOPYTHON_AVAILABLE:&#10;            return self.calculate_hydrophobicity_simple(sequence)&#10;        &#10;        try:&#10;            protein_analysis = ProteinAnalysis(str(sequence))&#10;            # 获取Kyte-Doolittle疏水性值的平均值&#10;            hydrophobicity = protein_analysis.protein_scale(&#10;                param_dict=self.kyte_doolittle, window=len(sequence)&#10;            )[0]&#10;            return hydrophobicity&#10;        except Exception:&#10;            return self.calculate_hydrophobicity_simple(sequence)&#10;    &#10;    def calculate_hydrophobicity_simple(self, sequence):&#10;        &quot;&quot;&quot;简化的疏水性计算&quot;&quot;&quot;&#10;        total_hydrophobicity = sum(self.kyte_doolittle.get(aa, 0) for aa in sequence)&#10;        return total_hydrophobicity / len(sequence)&#10;    &#10;    def calculate_amphiphilicity_advanced(self, sequence):&#10;        &quot;&quot;&quot;计算高级两亲性指标&quot;&quot;&quot;&#10;        # 基本统计&#10;        hydrophobic_count = sum(1 for aa in sequence if aa in self.hydrophobic_aa)&#10;        hydrophilic_count = sum(1 for aa in sequence if aa in self.hydrophilic_aa)&#10;        charged_count = sum(1 for aa in sequence if aa in self.charged_aa)&#10;        polar_count = sum(1 for aa in sequence if aa in self.polar_aa)&#10;        &#10;        total_length = len(sequence)&#10;        &#10;        # 计算比例&#10;        hydrophobic_ratio = hydrophobic_count / total_length&#10;        hydrophilic_ratio = hydrophilic_count / total_length&#10;        charged_ratio = charged_count / total_length&#10;        polar_ratio = polar_count / total_length&#10;        &#10;        # 获取平均疏水性&#10;        avg_hydrophobicity = self.calculate_hydrophobicity_biopython(sequence)&#10;        &#10;        # 计算两亲性指标&#10;        if hydrophobic_count == 0 or hydrophilic_count == 0:&#10;            amphiphilicity_score = 0&#10;        else:&#10;            # 方法1: 平衡度指标 (0-1)&#10;            balance = 1 - abs(hydrophobic_ratio - hydrophilic_ratio)&#10;            &#10;            # 方法2: 多样性指标 - 考虑疏水、亲水、带电的分布&#10;            diversity = min(hydrophobic_ratio, hydrophilic_ratio) * 2&#10;            &#10;            # 方法3: 基于疏水性分布的两亲性&#10;            # 理想的AMP应该有适中的疏水性（不太高不太低）&#10;            hydrophobicity_score = 1 - abs(avg_hydrophobicity) / 4.5  # 标准化到0-1&#10;            hydrophobicity_score = max(0, hydrophobicity_score)&#10;            &#10;            # 综合两亲性得分&#10;            amphiphilicity_score = (balance * 0.4 + diversity * 0.4 + hydrophobicity_score * 0.2)&#10;        &#10;        return {&#10;            'amphiphilicity_score': amphiphilicity_score,&#10;            'hydrophobic_ratio': hydrophobic_ratio,&#10;            'hydrophilic_ratio': hydrophilic_ratio,&#10;            'charged_ratio': charged_ratio,&#10;            'polar_ratio': polar_ratio,&#10;            'avg_hydrophobicity': avg_hydrophobicity,&#10;            'hydrophobic_count': hydrophobic_count,&#10;            'hydrophilic_count': hydrophilic_count,&#10;            'charged_count': charged_count&#10;        }&#10;    &#10;    def is_amphiphilic_advanced(self, sequence, min_amphiphilicity=0.3):&#10;        &quot;&quot;&quot;高级两亲性判断&quot;&quot;&quot;&#10;        amphiphilicity_data = self.calculate_amphiphilicity_advanced(sequence)&#10;        &#10;        # 条件1: 两亲性得分达标&#10;        score_pass = amphiphilicity_data['amphiphilicity_score'] &gt;= min_amphiphilicity&#10;        &#10;        # 条件2: 疏水性和亲水性氨基酸都要存在&#10;        both_present = (amphiphilicity_data['hydrophobic_count'] &gt; 0 and &#10;                       amphiphilicity_data['hydrophilic_count'] &gt; 0)&#10;        &#10;        # 条件3: 疏水性比例在合理范围内（20%-70%）&#10;        hydrophobic_balance = 0.2 &lt;= amphiphilicity_data['hydrophobic_ratio'] &lt;= 0.7&#10;        &#10;        # 条件4: 至少有一些带电氨基酸（用于抗菌活性）&#10;        has_charge = amphiphilicity_data['charged_count'] &gt;= 2&#10;        &#10;        # 条件5: 平均疏水性不能太极端&#10;        hydrophobicity_reasonable = -2.0 &lt;= amphiphilicity_data['avg_hydrophobicity'] &lt;= 2.0&#10;        &#10;        return (score_pass and both_present and hydrophobic_balance and &#10;                has_charge and hydrophobicity_reasonable)&#10;    &#10;    def calculate_additional_properties(self, sequence):&#10;        &quot;&quot;&quot;计算额外的蛋白质性质&quot;&quot;&quot;&#10;        additional_props = {}&#10;        &#10;        if BIOPYTHON_AVAILABLE:&#10;            try:&#10;                protein_analysis = ProteinAnalysis(str(sequence))&#10;                &#10;                # 分子量&#10;                additional_props['molecular_weight'] = protein_analysis.molecular_weight()&#10;                &#10;                # 等电点&#10;                additional_props['isoelectric_point'] = protein_analysis.isoelectric_point()&#10;                &#10;                # 芳香族氨基酸含量&#10;                additional_props['aromaticity'] = protein_analysis.aromaticity()&#10;                &#10;                # 不稳定性指数&#10;                additional_props['instability_index'] = protein_analysis.instability_index()&#10;                &#10;            except Exception as e:&#10;                print(f&quot;计算额外性质时出错: {e}&quot;)&#10;                additional_props = {&#10;                    'molecular_weight': None,&#10;                    'isoelectric_point': None,&#10;                    'aromaticity': None,&#10;                    'instability_index': None&#10;                }&#10;        else:&#10;            additional_props = {&#10;                'molecular_weight': None,&#10;                'isoelectric_point': None,&#10;                'aromaticity': None,&#10;                'instability_index': None&#10;            }&#10;        &#10;        return additional_props&#10;    &#10;    def apply_filters(self, sequences_df, &#10;                     min_net_charge=1, &#10;                     min_amphiphilicity=0.3,&#10;                     ph=7.4):&#10;        &quot;&quot;&quot;应用所有筛选条件&quot;&quot;&quot;&#10;        &#10;        results = []&#10;        &#10;        for idx, row in sequences_df.iterrows():&#10;            sequence = row['sequence'] if 'sequence' in row else row['Sequence']&#10;            &#10;            # 应用三个主要筛选条件&#10;            length_pass = self.calculate_length_filter(sequence)&#10;            net_charge = self.calculate_net_charge_biopython(sequence, ph)&#10;            charge_pass = net_charge &gt;= min_net_charge&#10;            amphiphilic_pass = self.is_amphiphilic_advanced(sequence, min_amphiphilicity)&#10;            &#10;            # 计算详细的两亲性信息&#10;            amphiphilicity_data = self.calculate_amphiphilicity_advanced(sequence)&#10;            &#10;            # 计算额外性质&#10;            additional_props = self.calculate_additional_properties(sequence)&#10;            &#10;            # 记录结果&#10;            result = {&#10;                'sequence': sequence,&#10;                'length': len(sequence),&#10;                'net_charge': net_charge,&#10;                'amphiphilicity_score': amphiphilicity_data['amphiphilicity_score'],&#10;                'hydrophobic_ratio': amphiphilicity_data['hydrophobic_ratio'],&#10;                'hydrophilic_ratio': amphiphilicity_data['hydrophilic_ratio'],&#10;                'charged_ratio': amphiphilicity_data['charged_ratio'],&#10;                'avg_hydrophobicity': amphiphilicity_data['avg_hydrophobicity'],&#10;                'length_pass': length_pass,&#10;                'charge_pass': charge_pass,&#10;                'amphiphilic_pass': amphiphilic_pass,&#10;                'all_pass': length_pass and charge_pass and amphiphilic_pass,&#10;                **additional_props&#10;            }&#10;            &#10;            # 保留原始数据的其他列&#10;            for col in row.index:&#10;                if col.lower() not in ['sequence']:&#10;                    result[f'original_{col}'] = row[col]&#10;            &#10;            results.append(result)&#10;        &#10;        return pd.DataFrame(results)&#10;    &#10;    def filter_and_save(self, input_file, output_file, &#10;                       min_net_charge=1, &#10;                       min_amphiphilicity=0.3,&#10;                       ph=7.4):&#10;        &quot;&quot;&quot;读取文件，应用筛选，保存结果&quot;&quot;&quot;&#10;        &#10;        # 检查Biopython&#10;        if BIOPYTHON_AVAILABLE:&#10;            print(&quot;使用Biopython进行精确计算&quot;)&#10;        else:&#10;            print(&quot;使用简化方法进行计算（建议安装Biopython获得更准确结果）&quot;)&#10;        &#10;        # 读取输入文件&#10;        if not os.path.exists(input_file):&#10;            raise FileNotFoundError(f&quot;输入文件不存在: {input_file}&quot;)&#10;        &#10;        try:&#10;            df = pd.read_csv(input_file)&#10;            print(f&quot;成功读取输入文件，共{len(df)}条序列&quot;)&#10;        except Exception as e:&#10;            raise Exception(f&quot;读取输入文件失败: {e}&quot;)&#10;        &#10;        # 应用筛选&#10;        results_df = self.apply_filters(df, min_net_charge, min_amphiphilicity, ph)&#10;        &#10;        # 统计结果&#10;        total_sequences = len(results_df)&#10;        passed_sequences = len(results_df[results_df['all_pass'] == True])&#10;        &#10;        print(f&quot;\n=== 筛选结果统计 ===&quot;)&#10;        print(f&quot;总序列数: {total_sequences}&quot;)&#10;        print(f&quot;通过长度筛选 (15-17 aa): {len(results_df[results_df['length_pass'] == True])}&quot;)&#10;        print(f&quot;通过电荷筛选 (净正电荷 &gt;= {min_net_charge}): {len(results_df[results_df['charge_pass'] == True])}&quot;)&#10;        print(f&quot;通过两亲性筛选 (得分 &gt;= {min_amphiphilicity}): {len(results_df[results_df['amphiphilic_pass'] == True])}&quot;)&#10;        print(f&quot;通过所有筛选条件: {passed_sequences}&quot;)&#10;        print(f&quot;筛选通过率: {passed_sequences/total_sequences*100:.2f}%&quot;)&#10;        &#10;        # 显示一些统计信息&#10;        if BIOPYTHON_AVAILABLE and len(results_df) &gt; 0:&#10;            print(f&quot;\n=== 序列性质统计 ===&quot;)&#10;            print(f&quot;平均分子量: {results_df['molecular_weight'].mean():.1f} Da&quot;)&#10;            print(f&quot;平均等电点: {results_df['isoelectric_point'].mean():.2f}&quot;)&#10;            print(f&quot;平均芳香族含量: {results_df['aromaticity'].mean():.3f}&quot;)&#10;        &#10;        # 保存所有结果&#10;        results_df.to_csv(output_file, index=False)&#10;        print(f&quot;\n详细结果已保存到: {output_file}&quot;)&#10;        &#10;        # 保存仅通过筛选的序列&#10;        passed_df = results_df[results_df['all_pass'] == True]&#10;        if len(passed_df) &gt; 0:&#10;            passed_output_file = output_file.replace('.csv', '_passed_only.csv')&#10;            passed_df.to_csv(passed_output_file, index=False)&#10;            print(f&quot;通过筛选的序列已保存到: {passed_output_file}&quot;)&#10;        &#10;        return results_df&#10;&#10;&#10;def main():&#10;    parser = argparse.ArgumentParser(description='改进的经验筛选抗菌肽候选序列')&#10;    parser.add_argument('--input', '-i', &#10;                       default='results/generated_peptides/candidate_amps.csv',&#10;                       help='输入CSV文件路径')&#10;    parser.add_argument('--output', '-o',&#10;                       default='results/generated_peptides/potential_amps.csv',&#10;                       help='输出CSV文件路径')&#10;    parser.add_argument('--min_charge', type=float, default=1.0,&#10;                       help='最小净正电荷 (默认: 1.0)')&#10;    parser.add_argument('--min_amphiphilicity', type=float, default=0.3,&#10;                       help='最小两亲性得分 (默认: 0.3)')&#10;    parser.add_argument('--ph', type=float, default=7.4,&#10;                       help='pH值 (默认: 7.4)')&#10;    &#10;    args = parser.parse_args()&#10;    &#10;    # 创建输出目录&#10;    os.makedirs(os.path.dirname(args.output), exist_ok=True)&#10;    &#10;    # 执行筛选&#10;    filter_engine = ImprovedExperienceFilter()&#10;    &#10;    try:&#10;        results_df = filter_engine.filter_and_save(&#10;            input_file=args.input,&#10;            output_file=args.output,&#10;            min_net_charge=args.min_charge,&#10;            min_amphiphilicity=args.min_amphiphilicity,&#10;            ph=args.ph&#10;        )&#10;        &#10;        print(f&quot;\n经验筛选完成！&quot;)&#10;        &#10;    except Exception as e:&#10;        print(f&quot;筛选过程中发生错误: {e}&quot;)&#10;        return 1&#10;    &#10;    return 0&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    import sys&#10;    sys.exit(main())" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/origin_data/blast.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/origin_data/blast.py" />
              <option name="originalContent" value="import pandas as pd&#10;import time&#10;import os&#10;from Bio.Blast import NCBIWWW, NCBIXML&#10;&#10;# === 1. 读取本地 CSV 文件 ===&#10;INPUT_FILE = &quot;Vespa.csv&quot;&#10;&#10;# 检查输入文件是否存在&#10;if not os.path.exists(INPUT_FILE):&#10;    print(f&quot;错误：输入文件 {INPUT_FILE} 不存在！&quot;)&#10;    exit(1)&#10;&#10;try:&#10;    df = pd.read_csv(INPUT_FILE)&#10;    print(f&quot;成功读取 {len(df)} 条记录&quot;)&#10;except Exception as e:&#10;    print(f&quot;读取文件失败：{e}&quot;)&#10;    exit(1)&#10;&#10;# === 2. 运行 NCBI BLASTp 在线比对（可替换为 DRAMP/APD3 API 如开放） ===&#10;def run_ncbi_blast(sequence):&#10;    try:&#10;        print(f&quot;  正在进行BLAST查询...&quot;)&#10;        result_handle = NCBIWWW.qblast(&quot;blastp&quot;, &quot;nr&quot;, sequence, format_type=&quot;XML&quot;, expect=10.0, hitlist_size=5)&#10;        blast_record = NCBIXML.read(result_handle)&#10;&#10;        if not blast_record.alignments:&#10;            return (&quot;无明显匹配&quot;, &quot;&lt;30%&quot;, &quot;无 MIC 报道&quot;, &quot;—&quot;, &quot;NCBI BLAST&quot;)&#10;&#10;        top_hit = blast_record.alignments[0]&#10;        hit_id = top_hit.hit_id&#10;        title = top_hit.title&#10;        score = top_hit.hsps[0].score&#10;        identity = top_hit.hsps[0].identities&#10;        align_len = top_hit.hsps[0].align_length&#10;        percent_identity = round(identity / align_len * 100, 1)&#10;&#10;        # 暂无 MIC 数据可查询（本地模拟）&#10;        mic_info = &quot;未知&quot;&#10;        target_bacteria = &quot;可能存在相关数据&quot;&#10;&#10;        return (title, f&quot;{percent_identity}%&quot;, mic_info, target_bacteria, &quot;NCBI BLAST&quot;)&#10;&#10;    except Exception as e:&#10;        print(f&quot;  BLAST查询失败：{e}&quot;)&#10;        return (&quot;查询失败&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, str(e))&#10;&#10;# === 3. 批量处理并记录结果 ===&#10;results = []&#10;total_records = len(df)&#10;&#10;for idx, row in df.iterrows():&#10;    name = row[&quot;Name&quot;]&#10;    seq = row[&quot;Sequence&quot;]&#10;    current_index = int(idx) if isinstance(idx, (int, float)) else idx&#10;    print(f&quot;查询进度 {current_index + 1}/{total_records}: {name} -&gt; {seq}&quot;)&#10;&#10;    try:&#10;        match, identity, mic, target, source = run_ncbi_blast(seq)&#10;        results.append([name, seq, match, identity, mic, target, source])&#10;        print(f&quot;  完成：相似度 {identity}&quot;)&#10;    except Exception as e:&#10;        print(f&quot;  处理失败：{e}&quot;)&#10;        results.append([name, seq, &quot;处理失败&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, str(e)])&#10;&#10;    # 控制请求频率，避免IP封锁&#10;    if current_index &lt; total_records - 1:  # 最后一个不需要等待&#10;        print(f&quot;  等待10秒...&quot;)&#10;        time.sleep(10)&#10;&#10;# === 4. 保存结果为文件 ===&#10;output_df = pd.DataFrame(results, columns=[&quot;Name&quot;, &quot;Sequence&quot;, &quot;Matched_Peptide&quot;, &quot;Similarity (%)&quot;, &quot;Known_MIC_Info&quot;, &quot;Target_Bacteria&quot;, &quot;Source_Database&quot;])&#10;&#10;try:&#10;    # 修复：使用正确的文件扩展名&#10;    output_file = &quot;vespa_peptide_blast_results.xlsx&quot;&#10;    output_df.to_excel(output_file, index=False)&#10;    print(f&quot;比对完成，结果已保存为 {output_file}&quot;)&#10;&#10;    # 同时保存为CSV格式作为备份&#10;    csv_file = &quot;vespa_peptide_blast_results.csv&quot;&#10;    output_df.to_csv(csv_file, index=False)&#10;    print(f&quot;CSV格式备份已保存为 {csv_file}&quot;)&#10;&#10;except Exception as e:&#10;    print(f&quot;保存Excel文件失败：{e}&quot;)&#10;    # 如果Excel保存失败，至少保存为CSV&#10;    try:&#10;        csv_file = &quot;vespa_peptide_blast_results.csv&quot;&#10;        output_df.to_csv(csv_file, index=False)&#10;        print(f&quot;已保存为CSV格式：{csv_file}&quot;)&#10;    except Exception as csv_e:&#10;        print(f&quot;CSV保存也失败：{csv_e}&quot;)&#10;" />
              <option name="updatedContent" value="import pandas as pd&#10;import time&#10;import os&#10;from Bio.Blast import NCBIWWW, NCBIXML&#10;&#10;# === 1. 读取本地 CSV 文件 ===&#10;INPUT_FILE = &quot;Vespa.csv&quot;&#10;&#10;# 检查输入文件是否存在&#10;if not os.path.exists(INPUT_FILE):&#10;    print(f&quot;错误：输入文件 {INPUT_FILE} 不存在！&quot;)&#10;    exit(1)&#10;&#10;try:&#10;    df = pd.read_csv(INPUT_FILE)&#10;    print(f&quot;成功读取 {len(df)} 条记录&quot;)&#10;except Exception as e:&#10;    print(f&quot;读取文件失败：{e}&quot;)&#10;    exit(1)&#10;&#10;# === 2. 运行 NCBI BLASTp 在线比对（可替换为 DRAMP/APD3 API 如开放） ===&#10;def run_ncbi_blast(sequence):&#10;    try:&#10;        print(f&quot;  正在进行BLAST查询...&quot;)&#10;        result_handle = NCBIWWW.qblast(&quot;blastp&quot;, &quot;nr&quot;, sequence, format_type=&quot;XML&quot;, expect=10.0, hitlist_size=5)&#10;        blast_record = NCBIXML.read(result_handle)&#10;&#10;        if not blast_record.alignments:&#10;            return (&quot;无明显匹配&quot;, &quot;&lt;30%&quot;, &quot;无 MIC 报道&quot;, &quot;—&quot;, &quot;NCBI BLAST&quot;)&#10;&#10;        top_hit = blast_record.alignments[0]&#10;        hit_id = top_hit.hit_id&#10;        title = top_hit.title&#10;        score = top_hit.hsps[0].score&#10;        identity = top_hit.hsps[0].identities&#10;        align_len = top_hit.hsps[0].align_length&#10;        percent_identity = round(identity / align_len * 100, 1)&#10;&#10;        # 暂无 MIC 数据可查询（本地模拟）&#10;        mic_info = &quot;未知&quot;&#10;        target_bacteria = &quot;可能存在相关数据&quot;&#10;&#10;        return (title, f&quot;{percent_identity}%&quot;, mic_info, target_bacteria, &quot;NCBI BLAST&quot;)&#10;&#10;    except Exception as e:&#10;        print(f&quot;  BLAST查询失败：{e}&quot;)&#10;        return (&quot;查询失败&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, str(e))&#10;&#10;# === 3. 批量处理并记录结果 ===&#10;results = []&#10;total_records = len(df)&#10;&#10;for idx, row in df.iterrows():&#10;    name = row[&quot;Name&quot;]&#10;    seq = row[&quot;Sequence&quot;]&#10;    current_index = int(idx) if isinstance(idx, (int, float)) else idx&#10;    print(f&quot;查询进度 {current_index + 1}/{total_records}: {name} -&gt; {seq}&quot;)&#10;&#10;    try:&#10;        match, identity, mic, target, source = run_ncbi_blast(seq)&#10;        results.append([name, seq, match, identity, mic, target, source])&#10;        print(f&quot;  完成：相似度 {identity}&quot;)&#10;    except Exception as e:&#10;        print(f&quot;  处理失败：{e}&quot;)&#10;        results.append([name, seq, &quot;处理失败&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, str(e)])&#10;&#10;    # 控制请求频率，避免IP封锁&#10;    if current_index &lt; total_records - 1:  # 最后一个不需要等待&#10;        print(f&quot;  等待10秒...&quot;)&#10;        time.sleep(10)&#10;&#10;# === 4. 保存结果为文件 ===&#10;output_df = pd.DataFrame(results, columns=[&quot;Name&quot;, &quot;Sequence&quot;, &quot;Matched_Peptide&quot;, &quot;Similarity (%)&quot;, &quot;Known_MIC_Info&quot;, &quot;Target_Bacteria&quot;, &quot;Source_Database&quot;])&#10;&#10;try:&#10;    # 只保存为CSV格式&#10;    csv_file = &quot;vespa_peptide_blast_results.csv&quot;&#10;    output_df.to_csv(csv_file, index=False)&#10;    print(f&quot;比对完成，结果已保存为 {csv_file}&quot;)&#10;&#10;except Exception as e:&#10;    print(f&quot;保存CSV文件失败：{e}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>